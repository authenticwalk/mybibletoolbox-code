# Community Discussions Tool: Proven Patterns & Learnings

**Status:** Extracted from 3 completed experiments (G1411, G1577, H430)
**Quality:** 100% L1, 100% L2, 88% L3 across all experiments
**Date:** 2025-11-15
**Experiments:** 3/3 complete (Etymological fallacy × 2, Theological projection × 1)

---

## Overview

This document catalogs **proven patterns** discovered through systematic experimentation with the Community Discussions tool. Unlike STAGES.md (execution workflow) or METHODOLOGY.md (tool-specific process), this captures **what actually works** based on empirical evidence.

**Source Material:**
- Experiment 1: G1411 δύναμις (dunamis → dynamite fallacy)
- Experiment 2: G1577 ἐκκλησία (ekklesia → "called out ones")
- Experiment 3: H430 אֱלֹהִים (Elohim → "plural proves Trinity")

---

## §1: Error Discovery Methods

### Pattern 1.1: WebSearch Query Templates

**Evidence:** All 3 experiments successfully discovered errors using consistent patterns.

**Proven Query Patterns:**

**For Finding Error Promotions:**
```
"{word}" + "{claim}" + "sermon" OR "devotional"
"{word}" + "etymology" + "teaching"
"{word}" + "means" + "{specific-claim}"
"Greek word for {concept}" + "{word}"
```

**Examples:**
- `"dunamis" + "dynamite" + "sermon"` → Found Harvest Ministries, FAMVIN
- `"ekklesia" + "called out ones" + "teaching"` → Found church identity materials
- `"Elohim" + "plural proves Trinity" + "apologetics"` → Found evangelism materials

**For Finding Refutations:**
```
"{word}" + "does not mean" + "{claim}"
"{word}" + "etymology" + "fallacy"
"exegetical fallacies" + "{word}"
"{word}" + "misconception" OR "error"
```

**Time Investment:** 15-20 minutes per error discovery

**Success Rate:** 100% (3/3 experiments found both promotions and refutations)

---

### Pattern 1.2: Source Type Hierarchy for Error Discovery

**Evidence:** Different source types reveal different error patterns.

**HIGH-VALUE SOURCES (Find both errors + refutations):**
- Stack Exchange Biblical Hermeneutics (moderated, scholarly answers)
- Reddit r/AcademicBiblical (academic standards enforced)
- B-Greek.org / B-Hebrew.org (expert correction often present)

**MEDIUM-VALUE SOURCES (Find errors, seek refutations elsewhere):**
- Devotional websites (Harvest Ministries, etc.)
- Christian teaching sites
- Sermon repositories
- Apologetics materials

**LOW-VALUE SOURCES (Document errors only, never cite as authority):**
- General Christian forums (errors perpetuated)
- Blog comments (reveal misconceptions)
- Social media discussions

**Learning:** Start with HIGH-VALUE sources for efficiency (both error + refutation in one place). Use MEDIUM/LOW sources to assess prevalence only.

---

### Pattern 1.3: Prevalence Assessment Signals

**Evidence:** Experiments showed clear patterns for assessing error spread.

**WIDESPREAD (G1411 dunamis → dynamite, G1577 ekklesia → "called out"):**
- Found in 5+ independent sources
- Appears in published materials (books, teaching curricula)
- Google autocomplete suggests the error
- Corrected in multiple scholarly sources (Carson, commentaries)
- Used in sermon illustrations repeatedly

**MODERATE (H430 Elohim → Trinity proof):**
- Found in 2-4 locations
- Recurring question on Stack Exchange
- Mentioned in apologetics contexts
- Some scholarly refutation exists

**UNCOMMON:**
- Single occurrence or rare
- Quickly corrected in community
- Not widely propagated

**Time Investment:** 5-10 minutes to assess prevalence (count sources, check autocomplete)

---

## §2: Refutation Strategies

### Pattern 2.1: Authority Tier Sources

**Evidence:** All 3 experiments used two-tier authority structure successfully.

**HIGH AUTHORITY (Use for refutation):**
- **D.A. Carson "Exegetical Fallacies"** - Gold standard (used in all 3 experiments)
- **BDAG Greek Lexicon** - Definitive for NT Greek (used in Exp 1, 2)
- **BDB Hebrew Lexicon** - Definitive for OT Hebrew (used in Exp 3)
- **LSJ Classical Greek Lexicon** - Authoritative for etymology (used in Exp 1, 2)
- **Peer-reviewed commentaries** - Douglas Moo, etc. (used in Exp 1)
- **Academic articles** - JSTOR, TheoLib (available but not yet used)

**MEDIUM AUTHORITY (Use as supplement):**
- **Expert blogs with credentials** - Mark Drinnenberg (Scribal Cafe), Drew Maust (Exp 1)
- **Seminary professor blogs** - Verified academic affiliation
- **Lexicon notes/warnings** - HELPS Word Studies, Thayer's notes
- **Credentialed ministry sites** - BibleProject (not yet used)

**LOW AUTHORITY (Never use for refutation):**
- Community forums (document errors only)
- Devotional sites (show prevalence only)
- Blog comments (identify misconceptions only)

**Verification Checklist for MEDIUM Authority:**
- [ ] Author has Ph.D. or M.Div.?
- [ ] Author cites scholarly sources?
- [ ] Author affiliated with seminary/institution?
- [ ] Blog demonstrates exegetical competence?

**If yes to 2+ → MEDIUM authority acceptable**
**If no to 3+ → LOW authority (don't use for refutation)**

**Success Rate:** 100% (all experiments found HIGH authority refutations)
**Time Investment:** 15-20 minutes to find scholarly refutations

---

### Pattern 2.2: Evidence Type Hierarchy

**Evidence:** Experiments showed different evidence types have different persuasive power.

**MOST COMPELLING:**

**1. Chronological Arguments (Exp 1: dunamis → dynamite)**
- **Power:** Irrefutable (ancient word cannot derive from modern invention)
- **Example:** NT written 50-100 CE, dynamite invented 1867 CE (1,800-year gap)
- **Use Case:** Anachronistic errors, reverse etymology
- **Time to Find:** 5-10 minutes (simple timeline research)

**2. Lexical Consensus (All experiments)**
- **Power:** Highly authoritative (3+ lexicons agree)
- **Example:** BDAG, LSJ, Thayer's all define ekklesia as "assembly"
- **Use Case:** Etymology disputes, semantic range errors
- **Time to Find:** 10-15 minutes (check multiple lexicons)

**STRONG:**

**3. Grammatical Evidence (Exp 3: Elohim plural)**
- **Power:** Technical but convincing (Hebrew grammar rules)
- **Example:** Plural of majesty + singular verbs = singular meaning
- **Use Case:** Theological projection, grammatical errors
- **Time to Find:** 15-20 minutes (consult grammar resources)

**4. Usage Counterexamples (Exp 2: ekklesia)**
- **Power:** Demonstrates range beyond single claim
- **Example:** Ekklesia used for secular assemblies in classical Greek
- **Use Case:** Over-specification, selective definition
- **Time to Find:** 10-15 minutes (corpus search)

**SUPPLEMENTARY:**

**5. Scholarly Consensus (All experiments)**
- **Power:** Appeals to authority (credible but not self-evident)
- **Example:** "Carson documents this as common fallacy"
- **Use Case:** All error types (establishes refutation exists)
- **Time to Find:** 5-10 minutes (cite Carson, commentaries)

**Learning:** Use 2-3 evidence types per refutation. Prioritize chronological (if applicable) + lexical consensus.

---

### Pattern 2.3: Multiple Refutation Sources Rule

**Evidence:** All experiments used 3-7 refutation sources (average: 5.3 sources/experiment).

**Minimum Standard:** 2+ refutation sources (HIGH or MEDIUM authority)
**Best Practice:** 3-5 sources (1-2 HIGH + 2-3 MEDIUM)
**Excellent:** 5+ sources (2-3 HIGH + 3+ MEDIUM)

**Why Multiple Sources:**
1. **Convergence:** Shows scholarly consensus, not isolated opinion
2. **Redundancy:** If one source becomes inaccessible, others remain
3. **Nuance:** Different sources emphasize different evidence types
4. **Credibility:** More persuasive than single source

**Example (Exp 1: dunamis):**
- HIGH: Carson (Exegetical Fallacies), BDAG, LSJ
- MEDIUM: Mark Drinnenberg (Scribal Cafe), Drew Maust blog, Douglas Moo commentary, Nick Norelli blog
- **Total:** 7 sources (3 HIGH, 4 MEDIUM)

**Time Investment:** +5-10 minutes per additional source (diminishing returns after 5 sources)

---

## §3: Schema Robustness & Design Principles

### Pattern 3.1: Schema Worked Without Refinement

**Evidence:** Schema used across 3 experiments with ZERO modifications needed.

**Key Schema Components Proven Effective:**

**1. Error → Refutation → Evidence Structure**
- Clear separation of LOW authority (error) from HIGH/MEDIUM (refutation)
- Evidence subsection provides proof for refutation
- Multiple error types accommodated

**2. Error Classification (Primary + Secondary + Contributing)**
- Exp 1: Primary=etymological_fallacy, Secondary=anachronism, Contributing=reverse_etymology
- Exp 2: Primary=etymological_fallacy (root fallacy subtype)
- Exp 3: Primary=theological_projection
- **Learning:** Most errors are compound; schema allows multiple type assignment

**3. Pedagogical Sections**
- `why_tempting` - Acknowledges human psychology (all 3 experiments used)
- `alternative_framing` - Positive teaching opportunity (all 3 experiments used)
- `helpful_cautions` - Prevents over-correction (all 3 experiments used)
- **Impact:** Gracious tone maintained, educational value high

**4. Common Questions Integration**
- Each experiment included 1-3 common questions
- Answers cite same HIGH authority sources
- **Value:** Addresses reader curiosity, prevents new errors

**5. Metadata Fields**
- Prevalence assessment (widespread/moderate/uncommon)
- Sensitivity level (low/medium/very high)
- Human review flag (Exp 3 flagged for Trinity sensitivity)
- **Use:** Enables filtering, prioritization, review workflows

**Success Rate:** 100% (no schema refinements needed across 3 diverse error types)

**Learning:** Schema is production-ready. Do NOT modify without strong evidence.

---

### Pattern 3.2: Two-Tier Authority Marking Critical

**Evidence:** All 3 experiments maintained clear authority distinction.

**LOW Authority (Error Documentation):**
- Devotional websites (Harvest Ministries)
- Teaching materials (church curricula)
- Apologetics sites (evangelism materials)
- Forum discussions (Stack Exchange questions, not answers)
- Blog posts (without credentials)

**HIGH/MEDIUM Authority (Refutation):**
- Lexicons (BDAG, BDB, LSJ, Thayer's)
- Scholarly books (Carson, commentaries)
- Peer-reviewed articles (JSTOR, TheoLib)
- Expert blogs (credentials verified)

**Critical Rule:** NEVER cite LOW authority source for refutation. ONLY use for error documentation.

**Verification Example (Exp 1):**
- ❌ "Harvest Ministries says dunamis ≠ dynamite" (LOW citing LOW)
- ✅ "BDAG shows dunamis predates dynamite by 1,800 years" (refutation cites HIGH)

**Impact:** Maintains credibility, prevents circular reasoning, enables trust

---

## §4: Tone & Sensitivity Guidelines

### Pattern 4.1: Gracious Tone Framework

**Evidence:** All 3 experiments passed tone validation (100% L1, 100% L2).

**Proven Phrasing Patterns:**

**ALWAYS USE:**
- ✅ "While this interpretation is common..." (Exp 1, 2, 3)
- ✅ "Though tempting to connect these..." (Exp 1, 2)
- ✅ "This claim appears in several sources, but evidence shows..." (Exp 1)
- ✅ "Chronology indicates..." (Exp 1)
- ✅ "Lexicons reveal..." (Exp 2)
- ✅ "Grammar demonstrates..." (Exp 3)

**NEVER USE:**
- ❌ "This ridiculous claim..."
- ❌ "Obviously wrong..."
- ❌ "Anyone who studied Greek knows..."
- ❌ "Only amateurs believe..."

**"Why Tempting" Section (Required):**
- Acknowledges psychological appeal of error
- Validates desire for deeper understanding
- Explains pedagogical attraction
- **Examples:**
  - Exp 1: "Etymology feels like unlocking secret meaning"
  - Exp 2: "Root analysis seems scholarly and memorable"
  - Exp 3: "Supporting doctrine with linguistic evidence is appealing"

**"Alternative Framing" Section (Required):**
- Provides positive teaching angle
- Separates valid theology from invalid linguistics
- Offers stronger evidence for same conclusion
- **Examples:**
  - Exp 1: Valid connection = OT background (dunamis in LXX)
  - Exp 2: Valid theology = believers ARE called, word doesn't MEAN "called out"
  - Exp 3: Valid doctrine = Trinity true, grammar argument weak; use NT evidence instead

**Impact:** Zero complaints about tone, educational value high, maintains respect

---

### Pattern 4.2: Sensitivity Scaling

**Evidence:** Experiments ranged from low (Exp 1) to very high (Exp 3) sensitivity.

**LOW SENSITIVITY (Exp 1: dunamis → dynamite):**
- Common sermon error, widely corrected
- No doctrinal implications
- Safe to correct directly
- **Tone:** Standard gracious phrasing sufficient

**MEDIUM SENSITIVITY (Exp 2: ekklesia → "called out ones"):**
- Affects church identity teaching
- Used in ecclesiology discussions
- Requires nuance (valid theology, invalid etymology)
- **Tone:** Separate theology from linguistics explicitly

**VERY HIGH SENSITIVITY (Exp 3: Elohim → Trinity proof):**
- Core Christian doctrine (Trinity)
- Used in apologetics, evangelism
- Emotional investment high
- **Tone:** EXTRA gracious, affirm doctrine while correcting argument

**Sensitivity Handling Checklist:**
- [ ] Flag for human review? (very high = yes)
- [ ] Separate theology from linguistics? (medium/high = yes)
- [ ] Provide stronger alternative evidence? (high = yes)
- [ ] Affirm valid doctrine explicitly? (very high = yes)
- [ ] Extra review by theological expert? (very high = recommended)

**Learning:** Scale tone graciously based on sensitivity. Doctrine + argument separation prevents offense.

---

### Pattern 4.3: Doctrine + Argument Separation

**Evidence:** Exp 2 and 3 successfully separated valid theology from invalid linguistic arguments.

**Framework:**

**Step 1: Acknowledge Valid Theology**
- Exp 2: "The church IS a called people (1 Peter 2:9)"
- Exp 3: "The Trinity IS biblically defensible and orthodox"

**Step 2: Identify Invalid Linguistic Argument**
- Exp 2: "But ekklesia doesn't MEAN 'called out ones' etymologically"
- Exp 3: "But Elohim's plural form doesn't PROVE Trinity (plural of majesty)"

**Step 3: Explain Why Distinction Matters**
- Exp 2: "Etymology ≠ usage meaning"
- Exp 3: "Grammar feature ≠ doctrinal proof"

**Step 4: Provide Stronger Evidence**
- Exp 2: "Ekklesia used for secular assemblies in classical Greek"
- Exp 3: "Use Matthew 28:19, John 1:1, 2 Corinthians 13:14 for Trinity"

**Impact:** Allows correction without offense, validates theology while refining argument

---

## §5: Integration Patterns

### Pattern 5.1: Tool 1 Lexicon-Core Integration (When Available)

**Evidence:** Experiments defined integration workflow (Tool 1 data unavailable during experiments).

**Planned Integration Workflow:**
```
Step 1: Read Tool 1 lexicon-core/{num}.yaml
Step 2: Extract etymology + semantic_range sections
Step 3: Compare Tool 1 data to community error claim
Step 4: Use Tool 1 data as refutation evidence (cite {lexicon-core})
Step 5: Cross-reference convergence (Tool 1 + BDAG + LSJ = strong)
Step 6: Note in integration section of community-discussions.yaml
```

**Expected Benefits:**
- Faster refutation (Tool 1 data already extracted)
- Stronger convergence (Tool 4 builds on Tool 1)
- Consistency across tools (same lexicon sources)

**Impact:** When Tool 1 available, estimated time reduction: 10-15 minutes/word

---

### Pattern 5.2: Tool 2-3 Integration (Optional but Valuable)

**Evidence:** Experiments identified potential integration points (Tools 2-3 not yet available).

**Use Case 1: Tool 2 Scholarly Analysis**
- Check if scholarly insights already address error
- Reuse refutation if present
- Cross-reference in both directions

**Use Case 2: Tool 3 Web Insights**
- Expert blogs often correct errors (Mark Drinnenberg, Drew Maust used in Exp 1)
- Extract refutations from Tool 3 data
- Avoid duplicate research

**Integration Note Format:**
```yaml
integration:
  tool_1_lexicon_core:
    status: "cross-referenced"
    findings: "Etymology section confirms chronological impossibility"
  tool_2_scholarly_analysis:
    status: "not_available"
  tool_3_web_insights:
    status: "checked"
    findings: "Expert blog by Drinnenberg already addresses error"
```

**Impact:** Reduces duplication, strengthens convergence, improves efficiency

---

## §6: Quality Validation Framework

### Pattern 6.1: 3-Level Validation Proven Effective

**Evidence:** All 3 experiments passed 100% L1, 100% L2, 88% L3.

**Level 1: CRITICAL (100% Required) - All experiments passed 7/7 criteria**

| Criterion | Exp 1 | Exp 2 | Exp 3 | Success Rate |
|-----------|-------|-------|-------|--------------|
| Error has real source | ✅ | ✅ | ✅ | 100% |
| Refutation HIGH/MEDIUM authority | ✅ | ✅ | ✅ | 100% |
| Evidence provided | ✅ | ✅ | ✅ | 100% |
| Error type classified | ✅ | ✅ | ✅ | 100% |
| No fabrication | ✅ | ✅ | ✅ | 100% |
| Gracious tone | ✅ | ✅ | ✅ | 100% |
| Sources in ATTRIBUTION.md | ✅ | ✅ | ✅ | 100% |

**Learning:** L1 criteria are achievable with consistent workflow. Zero failures.

---

**Level 2: HIGH PRIORITY (80%+ Target) - All experiments passed 9/9 criteria (100%)**

| Criterion | Exp 1 | Exp 2 | Exp 3 | Success Rate |
|-----------|-------|-------|-------|--------------|
| Prevalence assessed | ✅ | ✅ | ✅ | 100% |
| Multiple refutation sources | ✅ | ✅ | ✅ | 100% |
| Helpful guidance | ✅ | ✅ | ✅ | 100% |
| Related errors documented | ✅ | ✅ | ✅ | 100% |
| Integration checked | ✅ | ✅ | ✅ | 100% |
| Common questions answered | ✅ | ✅ | ✅ | 100% |
| Pedagogical notes | ✅ | ✅ | ✅ | 100% |
| Error classification detailed | ✅ | ✅ | ✅ | 100% |
| Helpful cautions | ✅ | ✅ | ✅ | 100% |

**Learning:** L2 criteria consistently achievable. Exceeded 80% target (100% actual).

---

**Level 3: MEDIUM PRIORITY (60%+ Target) - All experiments passed 7/8 criteria (88%)**

| Criterion | Exp 1 | Exp 2 | Exp 3 | Success Rate |
|-----------|-------|-------|-------|--------------|
| Comprehensive evidence types | ✅ | ✅ | ✅ | 100% |
| Scholarly debates distinguished | ✅ | ✅ | ✅ | 100% |
| Positive alternative framing | ✅ | ✅ | ✅ | 100% |
| Cross-tool integration | ❌ | ❌ | ❌ | 0% (Tool 1 unavailable) |
| Cultural/historical context | ✅ | ✅ | ✅ | 100% |
| Fair use documentation | ✅ | ✅ | ✅ | 100% |
| Metadata complete | ✅ | ✅ | ✅ | 100% |
| Experiment learnings | ✅ | ✅ | ✅ | 100% |

**Learning:** L3 88% pass rate. Only failure = external factor (Tool 1 unavailable), not methodology issue.

---

### Pattern 6.2: Level 4 Usefulness Validation (To Be Tested)

**Status:** NOT YET VALIDATED (Gap identified in AUDIT-REPORT.md)

**Planned Scenarios (From STAGES.md v2.0):**

**Bible Translator Scenario:**
- Role: Translator working on minority language
- Questions:
  - Would you copy this to translation notes? (Yes/No)
  - What data helped you make translation decisions?
  - What mistakes did this help you avoid?
  - What data was missing that you needed?

**Pastor Scenario:**
- Role: Pastor preparing sermon
- Questions:
  - Would you use this in sermon prep? (Yes/No)
  - What insights would you share with congregation?
  - What data was too technical?
  - What data sparked "aha" moments?

**Seminary Student Scenario:**
- Role: Student writing exegetical paper
- Questions:
  - Would you cite this in your paper? (Yes/No)
  - Are sources credentialed enough for academic work?
  - What data strengthened your argument?
  - What claims seemed unsubstantiated?

**Target:** 70%+ practitioners would use outputs in at least one scenario

**Action Required:** Test on 5-10 stellar examples (G1411, G1577, H430 + 2-7 more)

---

## §7: Efficiency & Workflow Optimization

### Pattern 7.1: Time Investment Per Word

**Evidence:** Tracked across all 3 experiments.

**Breakdown (Average: 80 minutes/word):**

**Error Discovery: 20 minutes**
- WebSearch for error promotions: 10 min
- Assess prevalence: 5 min
- Document error sources: 5 min

**Refutation Search: 15 minutes**
- Find HIGH authority sources: 10 min
- Find MEDIUM authority sources: 5 min

**YAML Creation: 45 minutes**
- Draft error section: 10 min
- Draft refutation section: 10 min
- Extract evidence: 5 min
- Write pedagogical sections: 10 min
- Common questions + cautions: 5 min
- Metadata + quality check: 5 min

**Total: 80 minutes (comprehensive file)**

**Optimizations Identified:**
- Citation templates (Carson, BDAG, LSJ) → save 10 min
- Batch similar error types → save 5-10 min
- Tool 1 integration (when available) → save 10-15 min
- **Optimized Time: 60-70 minutes/word**

---

### Pattern 7.2: Batching Similar Error Types

**Evidence:** Experiments tested same error type more than once (Exp 1, 2 both etymological).

**Benefits:**
- Reuse search patterns (WebSearch templates consistent)
- Reuse refutation sources (Carson, BDAG recurring)
- Reuse pedagogical framing ("why tempting" patterns similar)
- Mental context switching reduced

**Batching Strategy:**
- All etymological fallacies together (Exp 1, 2 pattern)
- All theological projections together (Exp 3 pattern)
- All over-specifications together (future)

**Estimated Time Savings:** 5-10 minutes per word after first in batch

---

### Pattern 7.3: Citation Template Strategy

**Evidence:** Same sources cited across experiments.

**High-Frequency Citations (Create Templates):**
- `{carson-fallacies}` - Used in all 3 experiments
- `{bdag}` - Used in Exp 1, 2 (Greek words)
- `{lsj}` - Used in Exp 1, 2 (classical Greek)
- `{bdb}` - Used in Exp 3 (Hebrew words)

**Template Format:**
```yaml
sources:
  carson-fallacies:
    code: "carson-fallacies"
    title: "Exegetical Fallacies"
    author: "D.A. Carson"
    year: 1996
    pages: "[specific pages]"
    url: "https://www.amazon.com/..."
    authority: "HIGH"
```

**Impact:** Save 10 minutes/word (no re-typing metadata)

---

## §8: Production Scaling Considerations

### Pattern 8.1: Coverage Strategy

**Evidence:** Experiments informed prioritization approach.

**High-Priority Words (100-150 words):**
- Well-documented errors (Carson's book, common sermon errors)
- Widespread prevalence (5+ sources)
- Abundant refutations (HIGH authority available)
- **Time:** 60-70 min/word (optimized with templates)
- **Examples:** G1411 (dunamis), G1577 (ekklesia), G26 (agape), G5479 (chara)

**Medium-Priority Words (200-250 words):**
- Good web coverage (Stack Exchange discussions)
- Moderate prevalence (2-4 sources)
- MEDIUM authority refutations available
- **Time:** 70-80 min/word
- **Examples:** Theological terms, common questions

**Low-Priority Words (150-200 words):**
- Opportunistic discoveries (during other tool work)
- Uncommon prevalence (rare but worth documenting)
- Limited refutation sources (MEDIUM authority)
- **Time:** 80-90 min/word

**Total Target:** 500 words
**Estimated Timeline:** 6-8 months @ 20 hrs/week

---

### Pattern 8.2: Production Stopping Rule

**Evidence:** STAGES.md v2.0 diminishing returns principle.

**Batch Validation (Every 50-100 Words):**
1. Measure L2/L3 pass rates
2. Compare to previous batch
3. Track time per word
4. Assess quality consistency

**Stopping Criteria:**
- ✅ Both L2 and L3 improvements <5% for consecutive batches
- ✅ Time per word stabilized (no further efficiency gains)
- ✅ Quality scores consistently 8.5+/10
- ✅ No new error types discovered

**When to Stop:**
- Methodology mature (no improvements)
- Coverage sufficient (major errors documented)
- Diminishing returns reached (<5% gains)

**Action:** Move to next tool in sequence

---

## §9: Known Limitations & Mitigations

### Limitation 9.1: Tool 1 Lexicon-Core Dependency

**Issue:** Cross-tool integration limited when Tool 1 unavailable
**Impact:** Cannot verify convergence, must use web lexicons directly
**Mitigation:** Use BDAG/LSJ/BDB via WebFetch/WebSearch (slower but effective)
**Future:** When Tool 1 available, re-check integration for 10-20 high-priority words

---

### Limitation 9.2: Rare Word Sparse Refutation Sources

**Issue:** Hapax legomena may lack scholarly refutation (not yet tested)
**Impact:** May not reach L2 criteria (multiple refutation sources)
**Mitigation:** Document error if found, but may need only 1-2 MEDIUM authority sources
**Future:** Test on rare word in next experiment batch

---

### Limitation 9.3: WebFetch Accessibility

**Issue:** Some websites block scraping or require authentication
**Impact:** Occasional refutation source inaccessible via WebFetch
**Mitigation:** Use multiple sources (if one fails, others succeed)
**Workaround:** Manual extraction + fair use compliance

---

## §10: Next Experiments Recommended

### Experiment 4: Rare Word Over-Interpretation

**Purpose:** Test error type 4 (over-specification) on hapax legomenon
**Challenge:** Limited scholarly refutation sources (rare word)
**Learning Goal:** How to handle sparse refutation availability
**Example:** G5497 χειραγωγός (cheiragogos) - only Acts 13:11 (hand-leader)

---

### Experiment 5: Legitimate Scholarly Debate

**Purpose:** Distinguish genuine debate from popular error
**Challenge:** When multiple scholarly views exist (not just error)
**Learning Goal:** Use `scholarly_debates` section vs. error refutation
**Example:** G1228 διάβολος (diabolos) - "slanderer" vs. "Satan" debate

---

### Experiment 6: False Cognate Error

**Purpose:** Test error type 3 (false cognates)
**Challenge:** Phonetic similarity without linguistic connection
**Learning Goal:** Refute sound-alike claims with etymology
**Example:** Claiming φιλέω (phileo) related to English "feel"

---

## Conclusion

**Methodology Status:** ✅ PRODUCTION-READY (pending Level 4 validation)

**Proven Patterns (10 sections):**
1. ✅ Error discovery methods (WebSearch templates, source hierarchy, prevalence)
2. ✅ Refutation strategies (authority tiers, evidence types, multiple sources)
3. ✅ Schema robustness (error→refutation→evidence, no refinements needed)
4. ✅ Tone & sensitivity (gracious framework, doctrine+argument separation)
5. ✅ Integration patterns (Tool 1-3 workflows defined)
6. ✅ Quality validation (L1:100%, L2:100%, L3:88%; L4 to be tested)
7. ✅ Efficiency optimization (80 min/word, optimizable to 60-70 min)
8. ✅ Production scaling (coverage strategy, stopping rule)
9. ✅ Known limitations (documented with mitigations)
10. ✅ Next experiments (rare words, debates, false cognates)

**Confidence Level:** HIGH (3/3 experiments successful, 100% L1/L2 compliance)

**Recommendation:** Proceed to production with pilot batch (20-50 high-priority words) after completing Level 4 usefulness validation.

---

**Document Status:** COMPLETE
**Last Updated:** 2025-11-15
**Source Experiments:** 3/3 (G1411, G1577, H430)
