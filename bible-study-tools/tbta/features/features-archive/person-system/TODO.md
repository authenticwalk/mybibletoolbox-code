# Person System Feature - TODO (Stage 6 Completion)

**Current Status**: Stage 5 (Testing & Refinement) - Incomplete
**Migration Date**: 2025-11-15
**Blocker**: Random test failure + Algorithm v2.1 untested + No validate.yaml

---

## Critical Issues to Resolve

### ‚úÖ Issue 1: Random Test Failure - ROOT CAUSE IDENTIFIED (2025-11-15)
**Problem**: Random test achieved only 60% (6/10) with Algorithm v1.0 (target: 80-90%)
- Adversarial test: 73% (8/11) ‚úì meets 60-70% target
- Random test: 60% (6/10) ‚úó FAILS 80-90% target
- **This is backwards** - random should beat adversarial by 15-25 points!

**Root Cause Analysis - COMPLETED**:
‚úÖ Systematic error analysis completed for all 4 failed verses
‚úÖ 3 common failure patterns identified:

**Pattern 1: Nested Quotations** (3/4 failures)
- Verses: 2 Kings 18:22, Jeremiah 3:22, Ezekiel 33:10
- Problem: Algorithm doesn't analyze inner quotes separately
- Example: Assyrian quoting Jerusalem's hypothetical "we trust in the LORD"
- Algorithm Gap: Rule 5.2 insufficient for nested quote handling

**Pattern 2: Genre-Specific Misapplication** (3/4 failures)
- Prophecy: Prayer within prophetic quote not recognized (Jer 3:22)
- Prophecy: Corporate lament speaker confused with prophet frame (Ezek 33:10)
- Epistle: Apostolic authority overfitted, missing shared identity (Phil 3:20)

**Pattern 3: Implicit vs. Explicit Markers**
- Algorithm excels at EXPLICIT markers (contrast, theological categories)
- Algorithm struggles with IMPLICIT patterns (shared citizenship, corporate confession)
- Random verses have more implicit patterns than adversarial verses

**Why Backwards Result Occurs**:
- Adversarial test designed with explicit edge cases ‚Üí Algorithm's strength
- Random test contains common implicit patterns ‚Üí Algorithm's weakness
- Training set (20 verses) over-represents explicit markers, under-represents nested quotes and implicit patterns

**Remaining Actions**:
- [ ] Document full analysis in experiments/ERROR-ANALYSIS-RANDOM.md
- [ ] Update experiments/LEARNINGS.md with 3 failure patterns
- [ ] Test whether Algorithm v2.1 (PROMPT3.md) fixes these blind spots

---

### üü° Issue 2: Algorithm v2.1 Testing - COMPLETED (2025-11-15)
**Status**: v2.1 tested ‚Üí 71.4% accuracy (FAILED 75-80% target)

**Test Results**:
- Accuracy: 15/21 (71.4%)
- vs. v1.0: +9 points (v1.0 was 62%)
- vs. Projection: MISSED 75-80% target by 3.6-8.6 points

**What Worked** (3/4 fixes):
‚úÖ Fix 2 (Invitation split): Luke 24:29 fixed
‚úÖ Fix 3 (Authority): James 3:1 fixed
‚úÖ Fix 4 (Outsider quote): 2 Kings 18:22 fixed

**What Broke** (1/4 fixes):
‚ùå Fix 1 (Prayer Rule Priority): Overeager trigger
  - Fixed: Psalm 44:1, Jonah 1:14 (direct address TO God)
  - Broke: Psalm 66:6 ("in him" - 3rd person reference)
  - Broke: Ezekiel 33:10 (quoted lament, not direct prayer)
  - Net gain: ZERO (should have been +2)

**Root Cause**: Rule 2.1 triggers on ANY God-reference, not just direct address

**Required Fix for v2.2**:
```
Rule 2.1 trigger should be:
IF direct address TO God (vocative or 2nd person)
AND NOT third-person reference ("he", "him")
AND NOT prepositional phrase ("in him", "with the LORD")
THEN EXCLUSIVE
```

**Projected v2.2 accuracy**: 17/21 = 81% ‚úÖ (meets threshold)

**Detailed Analysis**: `/plan/person-system-v2.1-validation/test-results.md`

**Remaining Actions**:
- [ ] Implement v2.2 with strict Rule 2.1 trigger
- [ ] Re-test on 21-verse test set
- [ ] If ‚â• 81%, proceed to validate.yaml generation (Issue 3)

---

### üö® Issue 3: No Validate Set (STAGE 6 BLOCKER)
**Problem**: validate.yaml does not exist
- Stage 6 requires 100 verses per value (200 total)
- Must be generated by subagent (main agent cannot see answers)
- Required for blind validation and peer review

**Required Actions**:
- [ ] **Use subagent** to generate validate.yaml:
  - Subagent accesses TBTA repository
  - Samples 100 inclusive + 100 exclusive verses
  - Stratifies by genre (OT/NT, narrative/epistle/poetry)
  - Includes edge cases and adversarial examples
  - Creates validate.yaml in experiments/
  - **Main agent receives only file path** (no contents)
- [ ] Verify validate.yaml structure matches train.yaml format
- [ ] Document sampling methodology in validate.yaml metadata

**Estimated Time**: 2 hours (subagent work)

---

## Stage 5 Completion Requirements

Before proceeding to Stage 6, the following MUST be completed:

### ‚úÖ Already Complete
- [x] PROMPT1.md (Algorithm v1.0) created and tested
- [x] PROMPT2.md (Algorithm v2.0) created
- [x] PROMPT3.md (Algorithm v2.1) created
- [x] External validation (98% agreement, 9 languages, 7 verses)
- [x] Locked predictions with git commit (f373646)
- [x] ANALYSIS.md documenting 12 approaches
- [x] LEARNINGS.md with initial error patterns

### ‚¨ú Still Required
- [ ] Test Algorithm v2.1 on 21-verse test set (see Issue 2)
- [ ] Analyze 5 random test failures systematically (see Issue 1)
- [ ] Update experiments/LEARNINGS.md with complete error analysis
- [ ] Document why random test failed and how to fix
- [ ] Determine optimal algorithm version (v2.1 or v2.2?)

**Estimated Time**: 5-7 hours total

---

## Stage 6 Requirements (After Stage 5 Complete)

Once Stage 5 issues resolved, proceed to Stage 6:

### 6.1: Generate Validate Set
- [ ] Use subagent to create validate.yaml (100 verses per value)
- [ ] Verify no data leakage (main agent never sees validate answers)

### 6.2: Blind Validation
- [ ] Subagent 1: Apply best algorithm to validate.yaml (blind)
- [ ] Subagent 2: Score predictions, return only:
  - Overall accuracy
  - Accuracy by value (inclusive vs exclusive)
  - List of error verse references (no details)
- [ ] Main agent analyzes error patterns (without seeing answers)

### 6.3: Peer Review (4 Critical Reviews)
- [ ] **Theological Review**: Does clusivity assignment respect biblical theology?
  - Check: Divine speech patterns theologically sound?
  - Check: Prayer rules align with Christian doctrine?
  - Check: No systematic theological bias?
- [ ] **Linguistic Review**: Does model comply with clusivity theory?
  - Check: Definitions match linguistic literature (Comrie, etc.)?
  - Check: Participant analysis methodology sound?
  - Check: Genre handling appropriate?
- [ ] **Methodological Review**: Was testing rigorous?
  - Check: Sample sizes adequate (200 validate + 20 train + 21 test)?
  - Check: Locked predictions maintained integrity?
  - Check: Blind testing properly conducted?
  - Check: Statistical significance of results?
- [ ] **Translation Practitioner Review**: Does this help real translators?
  - Test with 2-3 person-marking languages (Tagalog, Indonesian, etc.)
  - Check: Do predictions improve translation quality?
  - Check: Are ambiguous cases clearly marked?
  - Check: Is confidence calibration accurate?

### 6.4: Documentation
- [ ] Create experiments/TRANSLATOR-IMPACT.md:
  - Document practitioner testing results
  - Sample verses tested in target languages
  - Translator feedback (helps? hinders? accurate?)
  - Net benefit analysis
- [ ] Create experiments/TBTA-REVIEW.md:
  - Document perspective divergences (e.g., Genesis 1:26)
  - Explain why translation vs discourse analysis differ
  - Recommend TBTA team review if needed

### 6.5: Integration & Iteration
- [ ] Integrate peer review feedback
- [ ] Address any critical issues raised
- [ ] Iterate if needed (create v2.2, v2.3, etc.)
- [ ] Retest if algorithm changed

**Estimated Time**: 8-10 hours total

---

## Production Readiness Criteria

Feature is production ready when ALL criteria met:

### Accuracy Thresholds
- [ ] ‚â•95% on validate set (Stage 6 blind test)
- [ ] Random test meets 80-90% target (currently FAILS at 50-60%)
- [ ] Adversarial test maintains ‚â•60% (currently 73%, good)
- [ ] External validation ‚â•90% (currently 98%, excellent)

### Peer Review Sign-Off
- [ ] Theological review: PASSED
- [ ] Linguistic review: PASSED
- [ ] Methodological review: PASSED
- [ ] Translation practitioner review: PASSED (net benefit positive)

### Documentation Complete
- [ ] All experiments/ files finalized
- [ ] README.md updated with final accuracy
- [ ] LEARNINGS.md has complete error analysis
- [ ] TRANSLATOR-IMPACT.md demonstrates value
- [ ] TBTA-REVIEW.md addresses divergences

### Process Integrity
- [ ] Locked predictions maintained (git commits)
- [ ] Blind testing conducted (subagent validation)
- [ ] No data leakage (main agent never saw validate answers)
- [ ] Statistical rigor maintained (adequate sample sizes)

---

## Known Gaps & Limitations

### Sample Size Issues
- **Training**: Only 20 verses (ideally 100+)
- **Test**: Only 21 verses (11 adversarial + 10 random)
- **Impact**: Small samples reduce statistical confidence
- **Mitigation**: validate.yaml will add 200 verses for better statistics

### Overfitting Risk
- **Evidence**: Random test (50-60%) below adversarial (73%)
- **This is backwards** - suggests algorithm learned training specifics, not general patterns
- **Mitigation**: Must analyze failures and retest v2.1

### Untested Claims
- **Problem**: Algorithm v2.1 marked "production ready" without validation
- **Risk**: May not actually improve over v1.0
- **Mitigation**: Must test before proceeding to Stage 6

### TBTA Coverage
- **Current**: Only 2 verses checked against TBTA (Genesis 1:26, Genesis 42:21)
- **Issue**: 1 showed perspective divergence (valid, but needs documentation)
- **Needed**: Validate against more TBTA verses to identify systematic divergences

---

## Estimated Timeline to Completion

| Phase | Tasks | Time | Blocker? |
|-------|-------|------|----------|
| **Resolve Issue 2** | Test Algorithm v2.1 | 2-3 hrs | No |
| **Resolve Issue 1** | Analyze 5 random failures | 3-4 hrs | No |
| **Complete Stage 5** | Update LEARNINGS.md | 2 hrs | No |
| **Subtotal Stage 5** | | **7-9 hrs** | |
| **Generate validate.yaml** | Subagent work | 2 hrs | No |
| **Blind validation** | Subagent testing | 2 hrs | No |
| **4 Peer reviews** | External reviewers | 4-6 hrs | No |
| **Documentation** | TRANSLATOR-IMPACT, TBTA-REVIEW | 2 hrs | No |
| **Subtotal Stage 6** | | **10-12 hrs** | |
| **TOTAL** | | **17-21 hrs** | |

**Best Case**: 2-3 days of focused work
**Realistic**: 1 week with reviews and iterations

---

## Next Immediate Actions (Priority Order)

1. **TEST ALGORITHM v2.1** (2-3 hours)
   - Apply PROMPT3.md to test.yaml
   - Calculate actual vs projected accuracy
   - Document results

2. **ANALYZE RANDOM FAILURES** (3-4 hours)
   - Identify 5 failed verses
   - Systematic 6-step analysis
   - Look for patterns

3. **UPDATE LEARNINGS.md** (2 hours)
   - Complete error analysis
   - Document failure patterns
   - Recommend fixes

4. **GENERATE VALIDATE.YAML** (2 hours, subagent)
   - 100 inclusive + 100 exclusive verses
   - Stratified sampling
   - No main agent access

5. **PROCEED TO STAGE 6** (10-12 hours)
   - Blind validation
   - Peer reviews
   - Documentation
   - Integration

---

## Questions for Feature Owner

1. **Random Test Failure**: Should we:
   - Expand random test sample (10 ‚Üí 30+ verses)?
   - Regenerate training set with better distribution?
   - Accept 50-60% if validate set shows different pattern?

2. **Algorithm v2.1**: If testing shows no improvement:
   - Iterate to v2.2 immediately?
   - Return to v1.0 as baseline?
   - Generate entirely new approach?

3. **Validate Set**: Preferred sampling strategy:
   - Pure random from all TBTA person-system verses?
   - Stratified by genre (OT/NT, narrative/epistle)?
   - Mix of random + adversarial (similar to test.yaml)?

4. **Peer Review**: Preferred reviewers:
   - Internal team members?
   - External biblical scholars?
   - Professional translators in person-marking languages?

---

**Status**: Stage 5 incomplete due to 3 critical blockers
**Next Action**: Test Algorithm v2.1 (PROMPT3.md) on test.yaml
**Estimated Completion**: 1 week (17-21 hours of work)
**Production Ready**: NO (Stage 6 not started, random test failed)
