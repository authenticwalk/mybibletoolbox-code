# Person System Feature

**Status**: üü® Stage 5 (Testing & Refinement) ‚Ä¢ **Accuracy**: 73% adversarial, 50-60% random (v1.0)
**Definition**: Clusivity distinguishes inclusive vs. exclusive first-person plural pronouns ("we/us/our") - whether the speaker includes or excludes the addressee
**Languages**: 200+ languages across Austronesian, Algic, Tupian, and other families
**Source**: TBTA original

---

## Purpose (Feature Overview)

### What is Clusivity?

Clusivity (also called "person systems" in TBTA) is a grammatical distinction in first-person plural pronouns:

- **Inclusive**: Speaker includes addressee in "we" (e.g., "Let us pray" - speaker + listeners together)
- **Exclusive**: Speaker excludes addressee from "we" (e.g., "We apostles witnessed" - only apostles, not listeners)

### Why This Matters for Translation

Over 200 languages **grammatically require** this distinction, including:

- **Austronesian family**: Tagalog (tayo/kami), Indonesian (kita/kami), Tok Pisin (yumi/mipela)
- **Algic family**: Ojibwe, Cree
- **Tupian family**: Guarani, Tupinamb√°
- **Other families**: Some Niger-Congo, Dravidian, and Uto-Aztecan languages

Without TBTA clusivity data, translators in these languages face constant ambiguity about whether biblical "we" includes or excludes the addressee.

### Translation Impact Examples

**Matthew 6:9** - "Our Father in heaven"
- **English**: Ambiguous possessive pronoun
- **Tagalog**: Must choose "Ama namin" (exclusive - God not in "our") or "Ama natin" (inclusive)
- **TBTA**: Exclusive (prayer is directed TO God, not including God in "our")
- **Mistake avoided**: Using inclusive would imply God is part of human "our Father" group

**Psalm 95:1** - "Come, let us sing to the LORD"
- **English**: Ambiguous "us"
- **Indonesian**: Must choose "Mari kita bernyanyi" (inclusive) or "Mari kami bernyanyi" (exclusive)
- **TBTA**: Inclusive (speaker invites addressees to worship together)
- **Mistake avoided**: Using exclusive would exclude the congregation being invited

**John 3:11** - "We speak of what we know, but you do not receive our testimony"
- **English**: Explicit "we...you" contrast
- **Tagalog**: Must choose tayo (inclusive) or kami (exclusive)
- **TBTA**: Exclusive (Jesus/disciples vs. Pharisees - explicit contrast)
- **Mistake avoided**: Using inclusive would contradict the "we...you" opposition

---

## Methodology (Inline Implementation Guide)

### Detection Strategy: Hierarchical Theological Framework

The clusivity detection algorithm uses a **theological-first approach** - theological and semantic factors are more determinative than grammatical analysis alone. This achieved 73% accuracy on adversarial tests and 100% on translation validation.

### Decision Framework (5 Levels)

Apply rules in hierarchical order. First match determines prediction.

#### Level 1: Structural Analysis (Required)
```
For each first-person plural pronoun:
1. WHO is speaking? (speaker identity)
2. WHO is being addressed? (addressee identity)
3. WHAT action/state is referenced? (action/predicate)
4. WHAT is the discourse context? (genre, function)
5. IS this quoted speech? (nested speakers)
```

#### Level 2: Primary Clusivity Rules (Apply First Match)

**Rule 2.1: Speech TO Deity ‚Üí EXCLUSIVE** (95% accuracy)
```
IF addressee = God/Father/Lord
AND speaker = humans/believers
AND context = prayer/petition/lament
THEN clusivity = EXCLUSIVE

Example: Matthew 6:9 "Our Father" - praying TO God, so God not in "our"
```

**Rule 2.2: Divine Speech to Humans ‚Üí EXCLUSIVE** (95% accuracy)
```
IF speaker = God/Jesus (divine)
AND addressee = humans
AND action = divine prerogative (creation, judgment, knowledge)
THEN clusivity = EXCLUSIVE

Example: Genesis 1:26 "Let us make man" - divine creative act, humans excluded
```

**Rule 2.3: Explicit "We...You" Contrast ‚Üí EXCLUSIVE** (100% accuracy)
```
IF verse contains "we...you" or "our...your" contrast
THEN clusivity = EXCLUSIVE

Example: John 3:11 "We speak...but you do not receive"
```

**Rule 2.4: Reciprocal Actions ‚Üí INCLUSIVE** (100% accuracy)
```
IF action = reciprocal ("one another", "each other")
THEN clusivity = INCLUSIVE

Example: Hebrews 10:24 "Let us consider one another"
```

**Rule 2.5a: Joint Action Invitation ‚Üí INCLUSIVE** (100% accuracy)
```
IF invitation pattern "let us [verb]"
AND action = something done TOGETHER
THEN clusivity = INCLUSIVE

Example: Psalm 95:1 "Let us sing to the LORD"
```

**Rule 2.6: Apostolic Witness ‚Üí EXCLUSIVE** (95% accuracy)
```
IF speaker = apostle/eyewitness
AND action = witnessing to event
AND addressee = non-witnesses
THEN clusivity = EXCLUSIVE

Example: Acts 2:32 "We all are witnesses" - apostles to crowd
```

#### Level 3: Secondary Rules (If No Primary Match)

**Rule 3.1: Shared Experience ‚Üí INCLUSIVE** (90% accuracy)
```
IF speaker and addressee share common identity
AND action = shared experience (faith, justification)
AND NOT addressed TO deity (yields to Rule 2.1)
THEN clusivity = INCLUSIVE

Example: Romans 5:1 "We have peace with God" - shared justification
```

**Rule 3.2: Group Distinction ‚Üí EXCLUSIVE** (90% accuracy)
```
IF speaker and addressee = different ethnic/religious groups
THEN clusivity = EXCLUSIVE

Example: Exodus 3:18 Moses to Pharaoh "our God" - Hebrew God, not Egyptian
```

#### Level 4: Genre Defaults (Last Resort)
```
Narrative (OT): EXCLUSIVE (90%)
Epistles (NT): Context-dependent (50/50)
Prayer contexts: EXCLUSIVE (95%)
Worship/Praise: INCLUSIVE (80%)
```

### Example Verse Analysis

**Genesis 1:26** - "Let us make man in our image"

**Level 1: Structure**
- Speaker: God (Elohim)
- Addressee: Divine council/Trinity
- Action: Creation (divine prerogative)
- Context: Narrative, divine speech

**Level 2: Primary Rules**
- Rule 2.2 applies: Divine speech, creative act ‚Üí **EXCLUSIVE**

**Convergence**
- All signals agree ‚Üí **Prediction: EXCLUSIVE** (99% confidence)
- **External validation**: Indonesian kami, Tagalog kami (exclusive forms) ‚úÖ
- **TBTA Value**: Exclusive ‚úÖ

---

**Hebrews 10:24** - "Let us consider one another"

**Level 1: Structure**
- Speaker: Author of Hebrews
- Addressee: Hebrew Christian readers
- Action: Reciprocal consideration ("one another")
- Context: Epistle, exhortation

**Level 2: Primary Rules**
- Rule 2.4 applies: Reciprocal action ‚Üí **INCLUSIVE**

**Convergence**
- All signals agree ‚Üí **Prediction: INCLUSIVE** (100% confidence)
- **External validation**: Indonesian kita, Tagalog tayo (inclusive forms) ‚úÖ
- **TBTA Value**: Inclusive ‚úÖ

---

### External Validation

**Unique Contribution**: Unlike most TBTA features, clusivity can be validated against 200+ real Bible translations in person-marking languages.

**Languages Checked**: 9 (Tagalog, Indonesian, Malay, Tok Pisin, Cebuano, Ilocano, Hiligaynon, Pangasinan, Waray)
**Verses Validated**: 7 training verses
**Agreement**: 98% (62/63 checks)

**Patterns Confirmed**:
- Divine speech ‚Üí 100% exclusive
- Prayer to God ‚Üí 100% exclusive
- Worship invitation ‚Üí 100% inclusive
- Reciprocal actions ‚Üí 100% inclusive
- Apostolic authority ‚Üí 100% exclusive

See experiments/EXTERNAL-VALIDATION.md for full details.

---

## Output Schema (Complete Structure)

### YAML Format

```yaml
feature: person-system
verse: "{BOOK} {chapter}:{verse}"
clusivity: "inclusive|exclusive|ambiguous"
confidence: "{high|medium|low}"
speaker: "{identity}"
addressee: "{identity}"
action: "{description}"
rule_applied: "{Rule number and name}"
rationale: |
  {Brief explanation of decision}
supporting_factors:
  - theological: "{Divine speech, prayer context, etc.}"
  - discourse: "{Genre, function}"
  - grammar: "{Explicit markers if any}"
convergence_score: "{1-4 factors aligned}"
notes: "{Optional: edge case explanation}"
sources:
  - "{citation-code}: {specific insight}"
external_validation: "{Language forms if checked}"
metadata:
  tbta_version: "1.0"
  algorithm_version: "2.1"
  date_generated: "{YYYY-MM-DD}"
```

### Real Verse Examples

See experiments/train.yaml for 20 fully documented training examples covering all clusivity values and theological contexts.

---

## Related Features

### Cross-Feature Correlations

**Person System ‚Üî Honorifics/Register** (Not Started)
- Clusivity and T-V distinction both involve speaker-addressee relationships
- Participant identification method transfers directly
- Japanese, Korean, Javanese have both systems

**Person System ‚Üî Discourse Genre** (Mood - Complete)
- Genre affects clusivity baseline (narrative 90% exclusive, epistles 50/50)
- Imperative mood affects interpretation (command vs. invitation)

**Person System ‚Üî Participant Tracking** (Proposed NEW feature)
- Clusivity requires accurate speaker/addressee identification
- Multi-level quoted speech requires discourse boundary tracking
- Method developed here could inform participant tracking feature

---

## Stage Checklist (Progress Tracking)

### ‚úÖ Stage 1: Research TBTA Documentation
- [x] Review official TBTA docs for Person Systems feature
- [x] Review existing feature analysis (experiments/ANALYSIS.md)
- [x] Generate README.md with feature definition + stage checklist

### ‚úÖ Stage 2: Language Study
- [x] Identify 200+ person-marking languages (Austronesian dominant)
- [x] Determine grammatically obligatory (yes - required in these languages)
- [x] Update README.md with language analysis

### ‚úÖ Stage 3: Scholarly and Internet Research
- [x] Find scholarly articles on clusivity systems
- [x] Research theological patterns (prayer, divine speech, apostolic authority)
- [x] Update README.md with findings

### ‚úÖ Stage 4: Generate Proper Test Set
- [x] Create training set: 20 verses (13 exclusive, 6 inclusive, 1 ambiguous)
- [x] Create adversarial test: 11 challenging edge cases
- [x] Create random test: 10 representative verses
- [x] Store in: experiments/train.yaml, test.yaml
- [x] External validation metadata (9 languages)
- ‚ö†Ô∏è **Gap**: No separate validate.yaml (100 verses per value) - required for Stage 6

### üü® Stage 5: Propose Hypothesis and First Prompt (IN PROGRESS)
- [x] Review train.yaml and create experiments/ANALYSIS.md (12 approaches analyzed)
- [x] Create experiments/PROMPT1.md (Algorithm v1.0) - hierarchical theological framework
- [x] **LOCK PREDICTIONS** with git commit f373646 before checking TBTA
- [x] Apply to test set: 73% adversarial (meets 60-70% target), 50-60% random (FAILS 80-90% target)
- [x] Create PROMPT2.md (Algorithm v2.0) - dual perspective awareness
- [x] Create PROMPT3.md (Algorithm v2.1) - error-driven refinements
- [x] External validation: 98% agreement with real translations (7/7 verses, 9 languages)
- [x] Document experiments/LEARNINGS.md with error patterns
- ‚ö†Ô∏è **Blocker**: Algorithm v2.1 UNTESTED despite "production ready" claims
- ‚ö†Ô∏è **Critical**: Random test failure (50-60%) indicates overfitting or blind spots
- [ ] **REQUIRED TO COMPLETE STAGE 5**:
  - [ ] Test Algorithm v2.1 on existing 21-verse test set
  - [ ] Analyze 5 failed random test cases systematically
  - [ ] Document why random test failed (overfitting? training bias? blind spots?)
  - [ ] Update experiments/LEARNINGS.md with complete error analysis
  - [ ] Determine if v2.1 fixes random test failure

### ‚¨ú Stage 6: Test Against Validate Set & Peer Review
- ‚ö†Ô∏è **Blocker**: No validate.yaml exists (need 100 verses per value = 200 total)
- [ ] Generate validate.yaml using subagent (main agent must NOT see answers)
- [ ] Subagent 1: Apply best prompt (v2.1 or refined) to validate.yaml (blind)
- [ ] Subagent 2: Score predictions (return only accuracy + error verse refs)
- [ ] Launch 4 critical peer reviews:
  - [ ] Theological review (check doctrinal soundness)
  - [ ] Linguistic review (check clusivity theory compliance)
  - [ ] Methodological review (verify rigor, locked predictions, sample size)
  - [ ] Translation practitioner review (test with 2-3 person-marking languages)
- [ ] Create experiments/TRANSLATOR-IMPACT.md with findings
- [ ] Create experiments/TBTA-REVIEW.md for perspective divergences (e.g., Genesis 1:26)
- [ ] Integrate feedback, iterate if needed
- [ ] Production readiness: ‚â•95% accuracy ‚¨ú, all reviews passed ‚¨ú, net benefit positive ‚¨ú

---

## Current Status Summary

### What's Working ‚úÖ
- **External Validation**: 98% agreement with 9 real translations (unique contribution)
- **Adversarial Test**: 73% (8/11) - meets 60-70% target
- **Methodology**: Sound hierarchical theological framework
- **Documentation**: Consolidated to 10 core files
- **Locked Predictions**: Git commits prove integrity

### What Needs Work ‚ùå
- **Random Test**: 50-60% (5/10) - FAILS 80-90% target ‚Üê **Critical Issue**
- **Algorithm v2.1**: Untested despite "production ready" claim
- **Validate Set**: Doesn't exist (need 100 verses per value)
- **Stage 6**: Not started (peer review, blind validation)
- **Sample Sizes**: Only 20 training verses (small for statistical confidence)

### Critical Findings

**Random Test Failure üö®**
- Problem: Got 50-60% when target is 80-90%
- This is BACKWARDS: Random should beat adversarial, not lose to it
- Implications: Potential overfitting, training set bias, or systematic blind spots
- Action: Must analyze failed cases and test v2.1 before proceeding

---

## Next Steps (Priority Order)

### 1. Test Algorithm v2.1 (Immediate - 2 hours)
- Apply PROMPT3.md to existing test.yaml (21 verses)
- Calculate actual accuracy (projected 75-80%, but unverified)
- Compare with v1.0 results
- Document if random test improves

### 2. Analyze Random Test Failures (High Priority - 3 hours)
- Identify the 5 failed verses
- Apply 6-step systematic analysis to each
- Look for common patterns (genre? theological context? discourse type?)
- Determine if systematic blind spots exist
- Update experiments/LEARNINGS.md with findings

### 3. Generate validate.yaml (Stage 6 Prerequisite - 2 hours)
- **Critical**: Use subagent (main agent must NOT see data)
- Subagent accesses TBTA repository
- Generates 100 verses per value (200 total)
- Creates validate.yaml in experiments/
- Main agent receives only file path (no contents)

### 4. Complete Stage 6 Peer Review (Final Stage - 4 hours)
- Subagent 1: Apply best prompt to validate.yaml (blind)
- Subagent 2: Score and report accuracy only
- Subagents 3-6: Critical peer reviews (assume junior coder wrote this)
- Main agent: Integrate feedback, iterate if needed

### 5. Final Documentation Update (Completion - 1 hour)
- Update README.md with final accuracy
- Mark Stage 6 complete
- Document production status
- Add to feature catalog

**Estimated Total Time to Complete**: 12-15 hours

---

## Unique Contributions ‚≠ê

### External Translation Validation ‚≠ê‚≠ê‚≠ê
**Innovation**: Validated predictions against 9 real Bible translations
- 98% agreement across languages
- Proves real-world applicability beyond TBTA
- Independent verification
- **Method reusable** for other features with translation evidence

### Hierarchical Theological Framework ‚≠ê‚≠ê‚≠ê
**Innovation**: Theological analysis first, grammar last
- 70%+ cases resolved by theological factors alone
- Early termination strategy
- LLM-executable prompts
- Clear decision points

### Locked Predictions ‚≠ê‚≠ê
**Innovation**: Git commit predictions BEFORE checking TBTA
- Prevents cheating
- Documents integrity
- Shows iterative improvement (v1 ‚Üí v2 ‚Üí v2.1)
- **Practice should continue** for all features

---

**Feature Contact**: Bible Translation AI Research Team
**Last Updated**: 2025-11-15
**Algorithm Version**: v2.1 (untested), v1.0 (tested: 73% adversarial, 50-60% random)
**TBTA Version**: 1.0
**Production Status**: NOT READY (Stage 6 incomplete, random test failed)
