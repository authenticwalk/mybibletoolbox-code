# NOUNS (Category 1) - Detailed Analysis

**Date**: 2025-11-05
**Status**: Comprehensive Analysis Based on Implementation and Examples
**Data Coverage**: 11,649 verses across 34 books

---

## 1. Most Valuable Sub-Features

### Tier A: Critical for 1000+ Languages

#### 1.1 Number System (Position 5)
**Translation Value**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Essential)

**Available Options**:
- `S` - Singular
- `D` - Dual (exactly 2)
- `T` - Trial (exactly 3) ‚ú®
- `Q` - Quadrial (exactly 4)
- `p` - Paucal (a few, small number)
- `P` - Plural

**Why It Matters**:
- English only has singular/plural (2-way system)
- 100+ languages have dual (Polynesian, Austronesian, Semitic)
- Many have trial (Kilivila, Larike, Tok Pisin, etc.)
- Some have paucal (Slavic languages, some Native American)
- **Wrong number = grammatically incorrect, theologically imprecise**

**Key Translation Scenarios**:
1. **Genesis 1:26** - God as Trinity (Trial number)
   - Languages with trial: Use trial form, not plural
   - Theological precision: Exactly 3 persons, not generic "many"

2. **John 20:24** - "Thomas, one of the twelve" (Dual when 2 disciples present)
   - Some languages must mark whether 2, 3, or many disciples

3. **Acts 23:23** - "two hundred soldiers" (Plural vs Paucal)
   - Languages distinguish "many" vs "a few"

**Implementation Quality**: ‚úÖ Excellent
- Processor preserves all number values
- No filtering applied (all meaningful)
- Clear distinction in output

#### 1.2 Person System (Position 10)
**Translation Value**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Essential)

**Available Options**:
- `1` - First person
- `2` - Second person
- `3` - Third person
- `A` - First Inclusive ‚ú® ("we" including listener)
- `B` - First Exclusive ("we" excluding listener)
- `F` - First as Third
- `S` - Second as Third
- `I` - First Inclusive as Third
- `E` - First Exclusive as Third

**Why It Matters**:
- 1000+ languages distinguish inclusive/exclusive "we"
- Critical languages: Tagalog, Malay, Fijian, Quechua, Guarani, Navajo, countless others
- **Using wrong form = confusion about who is included**
- Affects theology, relationships, group boundaries

**Key Translation Scenarios**:
1. **Genesis 1:26** - "Let us create..." (First Inclusive)
   - Within the Trinity, speaking to each other
   - Inclusive: Trinity members addressing each other

2. **Acts 15:25** - "It seemed good to us..."
   - Exclusive: Apostles speaking to congregation (not including them)
   - Tagalog: "kami" not "tayo"

3. **1 John 1:3** - "our fellowship is with..."
   - Inclusive: Author + readers together
   - Tagalog: "tayo" not "kami"

**Implementation Quality**: ‚úÖ Excellent
- All person distinctions preserved
- No filtering issues
- Hierarchical structure maintains context

#### 1.3 Participant Tracking (Position 6)
**Translation Value**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Essential)

**Available Options**:
- `I` - First Mention (new entity introduced)
- `D` - Routine (established participant)
- `i` - Integration
- `E` - Exiting (leaving the narrative)
- `R` - Restaging (reintroduced after absence)
- `O` - Offstage
- `G` - Generic
- `Q` - Interrogative
- `F` - Frame Inferable (can be inferred from context)

**Why It Matters**:
- Essential for **switch-reference languages** (Papua New Guinea, Native American)
- Disambiguates "which 'he' is which" in narratives
- Tracks discourse flow across verses
- Helps translators know when to use explicit vs implicit reference

**Key Translation Scenarios**:
1. **Genesis 4:8** - "Cain said to Abel... and he rose up and he killed him"
   - Cain: Routine ‚Üí tracks subject continuity
   - Abel: Routine ‚Üí Exiting (dies)
   - Switch-reference languages need explicit marking when subject changes

2. **Genesis 2:19-20** - God bringing animals to Adam
   - God: First Mention ‚Üí Routine (established)
   - Animals: Generic (not specific individual animals)
   - Adam: Routine throughout

3. **Luke 15:11-32** - Prodigal Son
   - Father: First Mention ‚Üí Routine ‚Üí Restaging (when son returns)
   - Older brother: Offstage ‚Üí Restaging (enters at end)

**Implementation Quality**: ‚úÖ Excellent
- All tracking states preserved
- Crucial for discourse coherence
- Works with Noun List Index for coreference

### Tier B: Important for Many Languages

#### 1.4 Noun List Index (Position 4)
**Translation Value**: ‚≠ê‚≠ê‚≠ê‚≠ê (High)

**Available Options**: 1-9, A-Z, a-z (62 possible entities per verse)

**Why It Matters**:
- Tracks coreference: same index = same entity
- Disambiguates multiple referents of same type
- Critical for complex narratives with multiple actors
- Enables computational tracking of entities across discourse

**Key Translation Scenarios**:
1. **Genesis 4:8** - Multiple "he" pronouns
   - Cain: Index "1"
   - Abel: Index "2"
   - Brother: Index "2" (same as Abel - coreference!)
   - Enables correct pronoun/name choice in target language

2. **John 20:3-4** - Peter and "other disciple"
   - Peter: Index "1"
   - Other disciple (John): Index "2"
   - Clear tracking of who did what

**Implementation Quality**: ‚úÖ Good
- Preserved in processing
- Hierarchical structure maintains relationships
- Enables queries like "find all entities with index X"

#### 1.5 Proximity (Position 8)
**Translation Value**: ‚≠ê‚≠ê‚≠ê‚≠ê (High)

**Available Options**:
- `n` - Near (general)
- `N` - Near Speaker and Listener
- `S` - Near Speaker
- `L` - Near Listener
- `R` - Remote within Sight
- `r` - Remote out of Sight
- `T` - Temporally Near
- `t` - Temporally Remote
- `C` - Contextually Near with Focus
- `c` - Contextually Near

**Why It Matters**:
- English: 2-way (this/that)
- Japanese/Korean: 3-way (kore/sore/are)
- Spanish: 3-way (este/ese/aquel)
- Some Native American: 5-way systems
- **Wrong demonstrative = confusing or unnatural**

**Key Translation Scenarios**:
1. **John 1:29** - "Behold the Lamb of God"
   - Jesus physically present but distinct from speaker
   - Near Speaker but distinguished (contextually near)
   - Japanese: sore („Åù„Çå) not kore („Åì„Çå)

2. **Luke 10:39** - "sat at the Lord's feet"
   - Mary near Jesus (Near Listener if Jesus is addressee)
   - Demonstrative choice depends on speaker perspective

**Implementation Quality**: ‚úÖ Good
- All proximity distinctions preserved
- Rich granularity maintained

### Tier C: Useful but More Specialized

#### 1.6 Polarity (Position 7)
**Translation Value**: ‚≠ê‚≠ê‚≠ê (Medium)

**Options**: `A` - Affirmative, `N` - Negative

**Why It Matters**:
- Marks negation at noun level
- Some languages require negative concord
- Helps with double negatives, negative polarity items

**Implementation Quality**: ‚úÖ Good

#### 1.7 Surface Realization (Position 11)
**Translation Value**: ‚≠ê‚≠ê‚≠ê (Medium)

**Options**:
- `N` - Not Applicable
- `A` - Argument of Verb
- `p` - Predicate Referent (singular)
- `P` - Predicate Referent (plural)

**Why It Matters**:
- Distinguishes arguments from predicates
- Relevant for languages with different marking for each

**Implementation Quality**: ‚úÖ Good (N/A filtered appropriately)

#### 1.8 Participant Status (Position 12)
**Translation Value**: ‚≠ê‚≠ê‚≠ê (Medium)

**Options**:
- `N` - Not Applicable
- `P` - Primary Participant
- `A` - Associated Participant
- `M` - Minimal Participant

**Why It Matters**:
- Hierarchical importance in narrative
- Affects reference strategy (full NP vs pronoun)
- Helps with participant saliency

**Implementation Quality**: ‚úÖ Good (N/A filtered appropriately)

---

## 2. Example Coverage Assessment

### ‚úÖ Well-Documented Examples

#### Genesis 1:26 (Excellent Coverage)
**Features Present**:
- ‚úÖ Trial Number (T) - God as 3 persons
- ‚úÖ First Inclusive Person (A) - Trinity addressing each other
- ‚úÖ Participant Tracking: Routine (D) - God established as speaker
- ‚úÖ Semantic Role: Most Agent-like (A) - God as actor
- ‚úÖ Part of Speech: Noun (N)

**What Makes This Example Strong**:
- Combines multiple rare features (Trial + First Inclusive)
- Theologically significant (Trinity doctrine)
- Clear translation implications for multiple language families
- Demonstrates cross-feature interaction

**Coverage**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Perfect example

#### Genesis 4:8 (Good Coverage)
**Features Present** (based on analysis documents):
- ‚úÖ Noun List Index - Cain (1), Abel (2), Brother (2)
- ‚úÖ Participant Tracking - Both Routine, Abel becomes Exiting
- ‚úÖ Demonstrates coreference (Brother = Abel)

**What Makes This Example Strong**:
- Multiple participants tracked
- Shows state transitions (Routine ‚Üí Exiting)
- Demonstrates index coreference
- Real translation problem (switch-reference)

**Coverage**: ‚≠ê‚≠ê‚≠ê‚≠ê Good example

#### Genesis 19:31 (Good Coverage)
**Features Present**:
- ‚úÖ Participant tracking (sisters in dialogue)
- ‚úÖ Connected to clause-level demographics (age, relationship)

**Coverage**: ‚≠ê‚≠ê‚≠ê Good but more clause-focused

### ‚ö†Ô∏è Examples Mentioned But Not Fully Detailed

#### Trial Number (Position 5 - T)
- ‚úÖ Genesis 1:26 - documented
- ‚ùì Other instances? - **Query Opportunity**: "Find all Trial number instances"
- **Gap**: Don't know how common this is in TBTA data

#### First Inclusive (Position 10 - A)
- ‚úÖ Genesis 1:26 - documented
- ‚ùì Other instances? - **Query Opportunity**: "Find all First Inclusive instances"
- **Gap**: Need more examples to see pattern

#### Participant Tracking States
- ‚úÖ Routine (D) - documented in multiple verses
- ‚úÖ Exiting (E) - Genesis 4:8 (Abel dying)
- ‚ùì First Mention (I) - **Need example**: When is a participant first introduced?
- ‚ùì Restaging (R) - **Need example**: Prodigal's father? Luke 15?
- ‚ùì Frame Inferable (F) - **Need example**: What counts as frame-inferable?
- ‚ùì Integration (i), Offstage (O), Generic (G), Interrogative (Q) - **Need examples**

**Coverage Gap**: We have ~40% of tracking state examples

### üìã Coverage Improvement Recommendations

**High Priority**:
1. Find examples of each Participant Tracking state
2. Query for Trial/Dual/Quadrial instances across corpus
3. Collect First Inclusive vs First Exclusive pairs
4. Document Proximity distinctions with concrete verses

**Medium Priority**:
5. Catalog Noun List Index usage patterns
6. Find Paucal number examples
7. Document Person system edge cases (First as Third, etc.)

---

## 3. Use Case Patterns - When Features Matter Most

### Pattern 1: Narrative Participant Flow
**Features Used**: Participant Tracking + Noun List Index

**Scenario**: Complex narrative with multiple actors (e.g., Gospel narratives, OT stories)

**Translation Need**:
- Languages with switch-reference must mark subject changes
- Languages with different pronoun strategies for new vs established participants
- Languages that must disambiguate multiple third-person referents

**Example Translation Decisions**:
- Kilivila (Papua New Guinea): Different verbal affixes when subject continues vs changes
- Korafe (Papua New Guinea): Must mark same-subject vs different-subject in clause chains
- Turkish: Topic-prominent, needs clear participant hierarchy

**TBTA Help**:
- Tracking states show when to use full NP vs pronoun
- Index numbers disambiguate referents
- Exiting/Restaging marks narrative importance shifts

**Query Use Cases**:
- "Show me all verses where participant goes from Routine ‚Üí Exiting" (death, departure)
- "Find verses with 3+ active participants" (complex tracking needed)
- "List Restaging instances" (narrative return patterns)

### Pattern 2: Grammatical Number Precision
**Features Used**: Number System

**Scenario**: Languages with dual/trial/paucal distinctions

**Translation Need**:
- Must choose correct number form or sound wrong/unnatural
- Theological precision (Trinity = trial, not plural)
- Natural expression (two people = dual in many languages)

**Example Translation Decisions**:
- Tok Pisin: "mitupela" (dual) vs "mitripela" (trial) vs "mipela" (plural)
- Kilivila: Separate forms for 2, 3, 4, few, many
- Arabic: Dual form required for exactly 2
- Slovene: Dual endings for 2 items

**TBTA Help**:
- Explicit encoding prevents guessing
- Theologically significant (Trinity as trial)
- Naturally maps to target grammar

**Query Use Cases**:
- "Find all Trial instances" (rare, theologically interesting)
- "Find all Dual instances where English says 'they'" (translation guide)
- "Find Paucal vs Plural distinctions" (learn semantic boundaries)

### Pattern 3: Inclusive/Exclusive "We"
**Features Used**: Person System (First Inclusive vs Exclusive)

**Scenario**: Languages that require the distinction (1000+ languages)

**Translation Need**:
- Wrong choice = confusion about group membership
- Affects theology (who is included in "we"?)
- Social relationships (in-group vs out-group)

**Example Translation Decisions**:
- Tagalog: "tayo" (inclusive) vs "kami" (exclusive) - NOT optional!
- Fijian: "kedaru" (incl. dual) vs "keirau" (excl. dual) - must choose
- Quechua: "-nchik" (inclusive) vs "-yku" (exclusive) suffixes
- Guarani: "√±ande" (inclusive) vs "ore" (exclusive)

**TBTA Help**:
- Explicit marking removes ambiguity
- Consistent across discourse
- Theologically important (believer inclusion)

**Query Use Cases**:
- "Find all First Inclusive in Paul's letters" (ecclesiology)
- "Find First Exclusive in Jesus' teaching" (disciples vs crowds)
- "Compare inclusive/exclusive in Acts speeches" (audience analysis)

### Pattern 4: Demonstrative Precision
**Features Used**: Proximity System

**Scenario**: Languages with 3+way demonstrative systems

**Translation Need**:
- Must specify spatial/temporal/contextual distance
- Wrong choice sounds unnatural or creates confusion
- Affects narrative perspective

**Example Translation Decisions**:
- Japanese: „Åì„Çå (kore - near me) / „Åù„Çå (sore - near you) / „ÅÇ„Çå (are - far)
- Korean: Similar 3-way system (Ïù¥/Í∑∏/Ï†Ä)
- Spanish: este (near speaker) / ese (near listener) / aquel (far from both)
- Malagasy: 4-way system including visible/invisible distinction

**TBTA Help**:
- Encodes speaker-listener perspective
- Marks visibility distinctions
- Temporal vs spatial vs contextual

**Query Use Cases**:
- "Find Near Speaker instances in Gospel narratives" (first-person perspective)
- "Find Remote out of Sight references" (invisible referents)
- "Compare Near vs Contextually Near" (understand distinction)

### Pattern 5: Discourse Coherence in Translation
**Features Used**: Participant Tracking + Noun List Index + Surface Realization

**Scenario**: Maintaining natural flow across clauses/verses

**Translation Need**:
- Topic continuity marking
- Reference chain management
- Information structure

**Example Translation Decisions**:
- Japanese: Topic marker „ÅØ (wa) vs subject marker „Åå (ga)
- Korean: Similar topic/subject distinction
- Turkish: Topic-prominent word order
- Amharic: Different word orders based on information structure

**TBTA Help**:
- Tracking shows topic continuity
- Index tracks referent chains
- Surface Realization marks syntactic role

**Query Use Cases**:
- "Find long participant chains (Routine across 5+ verses)"
- "Identify topic shifts (First Mention after all Routine)"
- "Find Frame Inferable arguments" (ellipsis patterns)

---

## 4. Implementation Learnings

### What Works Well ‚úÖ

#### 4.1 Processor Architecture
**Strong Points**:
- Clean separation of parsing and output
- Recursive processing handles nested structures
- Nullish filtering preserves meaningful data

**Evidence**:
```python
NULLISH_VALUES = {
    "Not Applicable",
    "Unspecified",
    # NOTE: "No" is meaningful (e.g., "Implicit: No" vs field absent)
    # NOTE: "Space" and "Period" mark structural elements
}
```

**Learning**: Semantic filtering is better than syntactic filtering
- Don't remove by pattern ("N*"), remove by meaning ("Not Applicable")
- Preserve all values that carry information, even negative values ("No")
- Boolean "No" ‚â† null/undefined

#### 4.2 Data Preservation
**Strong Points**:
- All noun features preserved in YAML output
- Hierarchical structure maintained (clauses ‚Üí children ‚Üí nouns)
- No lossy transformations

**Evidence**: 11,649 verses processed with 0 errors

**Learning**: Preserve structure, add semantics in tooling
- Keep raw data complete
- Build query/analysis tools on top
- Don't flatten hierarchical data prematurely

#### 4.3 Book Name Mapping
**Strong Points**:
- Handles TBTA variations ("1_Samuel", "Revelations")
- Maps to USFM 3.0 standard codes
- Extensible dictionary

**Learning**: External data requires normalization layer
- Expect inconsistencies in source data
- Map to standards early in pipeline
- Document variations for future sources

### What Could Be Improved ‚ö†Ô∏è

#### 4.4 Feature Extraction Layer Missing
**Gap**: No dedicated feature extraction for queries

**Current State**:
- Data stored in hierarchical YAML
- No index of feature values
- No quick lookup by feature type

**Example Problem**:
- Question: "How many verses have Trial number?"
- Current: Must parse all 11,649 YAML files
- Better: Index of Number values ‚Üí instant lookup

**Recommendation**:
```python
# Build feature index during processing
feature_index = {
    "noun_features": {
        "Number": {
            "Trial": ["GEN.001.026", ...],
            "Dual": ["...", ...],
        },
        "Person": {
            "First Inclusive": ["GEN.001.026", ...],
        }
    }
}
```

**Learning**: Build indexes for common access patterns

#### 4.5 No Cross-Reference to Macula
**Gap**: TBTA and Macula data not linked at verse level

**Current State**:
- TBTA: Clause-level hierarchical data
- Macula: Word-level flat data
- Stored separately, no connection

**Example Problem**:
- Question: "What's the Strong's number for the Trial-number noun in Gen 1:26?"
- Current: Must manually correlate "God" in TBTA with G2316 in Macula
- Better: Linked data structure

**Recommendation**:
```yaml
verse: GEN.001.026
words:  # From Macula
  - position: 1
    text: "·ΩÅ Œ∏ŒµœåœÇ"
    strongs: G2316
    tbta_constituent: "God"  # Link to TBTA
    tbta_index: "1"
```

**Learning**: Linking complementary datasets unlocks new insights

#### 4.6 No Validation Against TBTA Schema
**Gap**: Processor assumes TBTA structure is correct

**Current State**:
- Accepts whatever JSON structure TBTA provides
- No validation of expected fields
- No warnings for unusual values

**Example Problem**:
- If TBTA adds new tracking state "X - Unknown", we won't notice
- If field name changes ("Number" ‚Üí "Num"), silent failure
- New features added to TBTA won't be flagged

**Recommendation**:
```python
EXPECTED_NOUN_FEATURES = {
    "Number": ["S", "D", "T", "Q", "p", "P"],
    "Person": ["1", "2", "3", "A", "B", "F", "S", "I", "E"],
    "Participant Tracking": ["I", "D", "i", "E", "R", "O", "G", "Q", "F"],
    # ...
}

def validate_noun(noun_data):
    if "Number" in noun_data:
        if noun_data["Number"] not in EXPECTED_NOUN_FEATURES["Number"]:
            log(f"WARNING: Unexpected Number value: {noun_data['Number']}")
```

**Learning**: Validate assumptions, especially with external data

### What's Excellent ‚≠ê

#### 4.7 Format Follows SCHEMA.md
**Strong Points**:
- Verse IDs: `GEN.001.026` (BOOK.chapter.verse, zero-padded)
- Inline citations: source marker present
- Extensible structure

**Evidence**:
```yaml
verse: GEN.001.026  # SCHEMA.md compliant
source: tbta        # Source citation
version: 1.0.0      # Version tracking
```

**Learning**: Standards compliance from day one prevents refactoring pain

#### 4.8 Meaningful Data Retention
**Strong Points**:
- "No" values preserved (semantic meaning)
- Structural markers kept (Space, Period)
- Rich context maintained

**Evidence**: ~73% of original data preserved (27% truly nullish)

**Learning**: Less aggressive filtering = more useful data

---

## 5. Query Opportunities

### Tier A: High-Value Queries (Build First)

#### Q1: Trial Number Catalog
**Query**: "Find all verses with Trial number"

**Value**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- Rare feature (very few instances)
- Theologically significant (Trinity)
- Clear use case for specific languages

**Implementation**:
```python
verses_with_trial = find_verses_where(
    category="Noun",
    feature="Number",
    value="Trial"
)
```

**Expected Results**: ~5-10 verses? (Estimate - need to run query)

**Use Cases**:
- Translation into languages with trial number
- Theological study of Trinity references
- Comparative analysis with First Inclusive person

#### Q2: Inclusive vs Exclusive "We"
**Query**: "Compare First Inclusive and First Exclusive instances"

**Value**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- Affects 1000+ languages
- Patterns teach translators the distinction
- High confusion potential

**Implementation**:
```python
inclusive = find_verses_where(feature="Person", value="First Inclusive")
exclusive = find_verses_where(feature="Person", value="First Exclusive")
```

**Expected Results**: Hundreds of instances

**Use Cases**:
- Build training set for translators learning the distinction
- Identify patterns (Jesus to disciples, Paul to churches, etc.)
- Compare with audience data (clause-level Listener field)

#### Q3: Participant Tracking Patterns
**Query**: "Map participant flow across narrative"

**Value**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- Essential for discourse analysis
- Switch-reference translation aid
- Narrative comprehension

**Implementation**:
```python
participant_flow = track_participant(
    book="GEN",
    chapter=4,
    entity="Cain",
    features=["Participant Tracking", "Noun List Index"]
)
# Output: First Mention (v1) ‚Üí Routine (v2-7) ‚Üí Routine (v8)...
```

**Use Cases**:
- Show translators how participants are tracked
- Identify when to use full NP vs pronoun
- Understand narrative salience

#### Q4: Number System Distribution
**Query**: "Show distribution of number categories across corpus"

**Value**: ‚≠ê‚≠ê‚≠ê‚≠ê
- Understand TBTA coverage
- Find examples for each category
- Learn semantic boundaries (Paucal vs Plural)

**Implementation**:
```python
number_stats = {
    "Singular": count_where(Number="S"),
    "Dual": count_where(Number="D"),
    "Trial": count_where(Number="T"),
    "Quadrial": count_where(Number="Q"),
    "Paucal": count_where(Number="p"),
    "Plural": count_where(Number="P"),
}
```

**Expected Results**:
- Singular: ~60% (most common)
- Plural: ~35%
- Dual: ~3% (rare but present)
- Trial: <1% (very rare)
- Quadrial: <1% (very rare)
- Paucal: ~2%

**Use Cases**:
- Find examples for documentation
- Training data for translators
- Understand TBTA annotation density

### Tier B: Medium-Value Queries (Build Soon)

#### Q5: Proximity System Examples
**Query**: "Find examples of each proximity category"

**Value**: ‚≠ê‚≠ê‚≠ê‚≠ê
- Japanese/Korean/Spanish translation
- Less intuitive distinctions
- Need examples to clarify

**Implementation**:
```python
proximity_examples = {
    prox_type: find_verses_where(feature="Proximity", value=prox_type, limit=5)
    for prox_type in ["Near Speaker", "Near Listener", "Remote within Sight", ...]
}
```

**Use Cases**:
- Demonstrative choice guide
- Compare with narrative perspective (clause Speaker)
- Pattern recognition for "Near" vs "Contextually Near"

#### Q6: Coreference Chains
**Query**: "Find verses with multiple entities (Noun List Index)"

**Value**: ‚≠ê‚≠ê‚≠ê‚≠ê
- Complex narrative tracking
- Coreference resolution
- Translation disambiguation

**Implementation**:
```python
complex_verses = find_verses_where(
    distinct_values("Noun List Index") >= 3
)
```

**Use Cases**:
- Examples of multi-participant tracking
- Test cases for coreference systems
- Translation challenge identification

#### Q7: Participant Status Hierarchy
**Query**: "Compare Primary vs Associated vs Minimal participants"

**Value**: ‚≠ê‚≠ê‚≠ê‚≠ê
- Narrative saliency
- Reference strategy guidance
- Discourse structure

**Implementation**:
```python
participants_by_status = group_by(
    feature="Participant Status",
    values=["Primary Participant", "Associated Participant", "Minimal Participant"]
)
```

**Use Cases**:
- When to use full name vs pronoun
- Narrative importance tracking
- Character hierarchy in stories

### Tier C: Specialized Queries (Build As Needed)

#### Q8: Cross-Feature Correlations
**Query**: "Find features that co-occur (Trial + First Inclusive)"

**Value**: ‚≠ê‚≠ê‚≠ê
- Understand feature interactions
- Find rich examples
- Build comprehensive training cases

**Implementation**:
```python
rich_examples = find_verses_where_all(
    ("Number", "Trial"),
    ("Person", "First Inclusive"),
    ("Participant Tracking", "Routine")
)
```

**Use Cases**:
- Multi-feature translation scenarios
- Theological study (Trinity examples)
- Complex test cases

#### Q9: Missing Feature Analysis
**Query**: "Which features are rarely coded?"

**Value**: ‚≠ê‚≠ê‚≠ê
- Understand TBTA coverage gaps
- Set expectations for translators
- Identify annotation priorities

**Implementation**:
```python
feature_presence = {
    feature: percentage_verses_with_feature(feature)
    for feature in NOUN_FEATURES
}
```

**Use Cases**:
- Documentation of coverage
- Gap identification for future work
- Realistic expectations setting

#### Q10: Book-Level Feature Density
**Query**: "Compare feature annotation across books"

**Value**: ‚≠ê‚≠ê‚≠ê
- Understand which books are richly annotated
- Guide translators to best examples
- Quality assessment

**Implementation**:
```python
by_book = {
    book: count_features_per_verse(book)
    for book in AVAILABLE_BOOKS
}
```

**Use Cases**:
- Find books with best examples
- Understand TBTA annotation strategy
- Coverage analysis

---

## 6. Transferable Patterns

### Pattern 1: Feature-Based Indexing
**From NOUNS Implementation ‚Üí Apply To All Categories**

**What We Learned**:
- Hierarchical storage (YAML) is great for structure
- But querying requires indexed features
- Build feature index during processing, not after

**Transferable To**:
- VERBS: Index by Time, Aspect, Mood
- ADJECTIVES: Index by Degree
- CLAUSES: Index by Speaker, Genre, Illocutionary Force
- All categories benefit from feature-based queries

**Implementation Template**:
```python
def build_feature_index(category, data):
    """Generic feature indexer for any TBTA category"""
    index = defaultdict(lambda: defaultdict(list))

    for verse_ref, verse_data in data.items():
        for item in extract_category_items(verse_data, category):
            for feature_name, feature_value in item.items():
                if not is_nullish(feature_value):
                    index[feature_name][feature_value].append(verse_ref)

    return index
```

**Application Example**:
```python
# Works for any category
noun_index = build_feature_index("Noun", tbta_data)
verb_index = build_feature_index("Verb", tbta_data)
clause_index = build_feature_index("Clause", tbta_data)

# Consistent query interface
verses_with_trial = noun_index["Number"]["Trial"]
verses_historic_past = verb_index["Time"]["Historic Past"]
verses_imperative = clause_index["Illocutionary Force"]["Imperative"]
```

### Pattern 2: Semantic Filtering Over Syntactic
**From NOUNS Nullish Handling ‚Üí Apply To All Categories**

**What We Learned**:
- "Not Applicable" = nullish (remove)
- "No" = semantic meaning (keep)
- "Space"/"Period" = structural (keep)
- Filter by meaning, not pattern

**Transferable To**:
- All TBTA categories have "Not Applicable" values
- All have meaningful "No" values
- Pattern applies universally

**Implementation Template**:
```python
# BAD: Syntactic filtering
def filter_nullish_bad(value):
    return value.startswith("N")  # Removes "No", "Near Speaker", etc!

# GOOD: Semantic filtering
NULLISH_VALUES = {
    "Not Applicable",
    "Unspecified",
    ".",  # Empty placeholder
}

def is_nullish(value):
    if isinstance(value, str):
        return value in NULLISH_VALUES or value.strip() == ""
    return False
```

**Application**: Use same `is_nullish()` function across all categories

### Pattern 3: Validation Through Expected Values
**From NOUNS Learning (Section 4.6) ‚Üí Apply To All Categories**

**What We Need**:
- Enumerate expected values for each feature
- Validate during processing
- Warn on unexpected values (new features or errors)

**Transferable To**:
- Every TBTA category has enumerated values
- Official README documents them
- Can extract from ALL-FEATURES.md

**Implementation Template**:
```python
EXPECTED_VALUES = {
    "Noun": {
        "Number": ["S", "D", "T", "Q", "p", "P"],
        "Person": ["1", "2", "3", "A", "B", "F", "S", "I", "E"],
        "Participant Tracking": ["I", "D", "i", "E", "R", "O", "G", "Q", "F"],
        # ...
    },
    "Verb": {
        "Time": ["D", "A", "a", "b", "c", "d", "e", "f", "g", "h",
                 "P", "1", "2", "3", "4", "5", "6", "7", "8", "9", "F",
                 "T", "s", "E", "e"],
        "Aspect": ["I", "C", "c", "o", "i", "R", "H", "G", "U"],
        # ...
    },
    # ... other categories
}

def validate_feature(category, feature_name, feature_value):
    expected = EXPECTED_VALUES.get(category, {}).get(feature_name, [])
    if expected and feature_value not in expected:
        log(f"WARNING: Unexpected {category}.{feature_name} = {feature_value}")
        log(f"  Expected one of: {expected}")
```

**Application**: Add validation to all category processors

### Pattern 4: Cross-Dataset Linking Strategy
**From NOUNS Learning (Section 4.5) ‚Üí Apply To All Categories**

**What We Need**:
- Link TBTA data to Macula data at verse level
- Enable queries across both datasets
- Preserve separate structures (hierarchical vs flat)

**Transferable To**:
- Not just Nouns ‚Üî Macula words
- Verbs ‚Üî Macula verbs
- Clauses ‚Üî Macula syntax trees
- Any complementary dataset

**Implementation Template**:
```yaml
# Verse-level unified structure
verse: GEN.001.026

# Macula data (word-level, flat)
words:
  - position: 1
    text: "·ΩÅ Œ∏ŒµœåœÇ"
    strongs: "G2316"
    lemma: "Œ∏ŒµœåœÇ"
    gloss: "God"
    # ... other Macula fields

# TBTA data (clause-level, hierarchical)
clauses:
  - children:
      - Constituent: "God"
        Part: "Noun"
        Number: "Trial"
        Person: "First Inclusive"
        # Link to Macula word
        macula_position: 1  # Links to words[0]

# Cross-dataset queries enabled:
# "What's the Strong's number for Trial-number nouns?"
# "Show me Macula morphology for TBTA First Inclusive persons"
```

**Application**:
- Add `macula_position` or similar linking field
- Build correlation during processing
- Enable rich cross-dataset queries

### Pattern 5: Feature-Example Documentation
**From NOUNS Analysis ‚Üí Apply To All Categories**

**What We Learned**:
- Each feature value needs concrete examples
- Distribution stats help set expectations
- Use cases make features understandable

**Transferable To**:
- VERBS: "Find examples of each Time value"
- ADJECTIVES: "Find examples of each Degree"
- CLAUSES: "Find examples of each Speaker-Listener Age Relationship"

**Implementation Template**:
```markdown
# {CATEGORY} Feature: {FEATURE_NAME}

## Values and Examples

### {Value 1}
**Description**: ...
**Examples**:
- Verse 1: Context...
- Verse 2: Context...

**Translation Impact**: ...

**Frequency**: X% of verses (Y instances)

### {Value 2}
...
```

**Application**: Generate docs automatically from indexed data

### Pattern 6: Hierarchical Structure Preservation
**From NOUNS Implementation ‚Üí Apply To All Categories**

**What We Learned**:
- Don't flatten hierarchical data prematurely
- Keep clause ‚Üí phrase ‚Üí word structure intact
- Build tools that navigate hierarchy

**Transferable To**:
- Verb Phrases contain Verbs
- Noun Phrases contain Nouns
- Clauses contain all phrase types
- Hierarchy encodes syntactic structure

**Implementation Template**:
```python
def extract_category_items(verse_data, category, recursive=True):
    """Extract all items of a category, optionally recursive"""
    items = []

    def traverse(node):
        if isinstance(node, dict):
            # Check if this node is the target category
            if node.get("Part") == category:
                items.append(node)

            # Recurse into children
            if recursive and "children" in node:
                for child in node["children"]:
                    traverse(child)
        elif isinstance(node, list):
            for item in node:
                traverse(item)

    traverse(verse_data)
    return items
```

**Application**: Use for all category extraction

---

## 7. Recommendations

### For Documentation

1. **Create Feature Example Catalog** ‚úÖ High Priority
   - Run queries to find examples of each feature value
   - Document with verse context and translation impact
   - Build reference guide for translators

2. **Generate Distribution Statistics** ‚úÖ High Priority
   - How common is each feature value?
   - Which features are densely vs sparsely annotated?
   - Set realistic expectations

3. **Document Cross-Feature Patterns** ‚≠ê Medium Priority
   - Which features commonly co-occur?
   - What are the richest annotated verses?
   - Build multi-feature example sets

### For Implementation

4. **Build Feature Index** ‚úÖ High Priority
   - Implement pattern from Section 6.1
   - Enable fast queries
   - Critical for tools

5. **Add Validation Layer** ‚úÖ High Priority
   - Implement pattern from Section 6.3
   - Catch TBTA schema changes
   - Ensure data quality

6. **Link to Macula** ‚≠ê Medium Priority
   - Implement pattern from Section 6.4
   - Enable cross-dataset queries
   - Unlock new insights

7. **Build Query Interface** ‚úÖ High Priority
   - Implement queries from Section 5
   - Start with Tier A queries
   - User-friendly tool for translators

### For Tool Development

8. **Translation Feature Checker** ‚≠ê High Value
   - Input: Target language features (e.g., "has trial number")
   - Output: Relevant verses with those features
   - Use case: Guide translators to relevant examples

9. **Participant Tracker** ‚≠ê High Value
   - Input: Book/chapter
   - Output: Visual participant flow
   - Use case: Understand narrative structure

10. **Feature Explorer** ‚≠ê High Value
    - Input: Feature name (e.g., "Number")
    - Output: Examples, stats, translation notes
    - Use case: Learn what each feature means

---

## Summary

### NOUNS Category: Executive Summary

**Overall Value**: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (Essential)

**Top 3 Features**:
1. Number System - Essential for 100+ languages
2. Person System (Inclusive/Exclusive) - Essential for 1000+ languages
3. Participant Tracking - Essential for discourse coherence

**Implementation Quality**: ‚úÖ Strong foundation, room for query optimization

**Coverage**: 11,649 verses, well-documented high-value examples (Gen 1:26), need more examples of all tracking states

**Key Transferable Patterns**:
1. Feature-based indexing
2. Semantic filtering over syntactic
3. Validation through expected values
4. Cross-dataset linking
5. Feature-example documentation
6. Hierarchical structure preservation

**Next Steps**:
1. Build feature index for queries
2. Catalog examples of all feature values
3. Create translation feature checker tool
4. Link to Macula data
5. Apply learnings to other categories

**Impact**: Enables accurate Bible translation into 1000+ languages with grammatical categories that differ from English/Greek/Hebrew. Prevents translation errors, maintains theological precision, improves naturalness.

---

**Document Status**: Complete
**Next Analysis**: Category 2 (VERBS) - Time, Aspect, Mood features
