#!/usr/bin/env python3
"""
TBTA-to-Strong's Extraction Script
====================================

Extracts TBTA linguistic nodes for each Strong's word and generates YAML files.

This script implements a three-way join linking:
    TBTA linguistic features → Macula source text → Strong's numbers

Usage:
    python extract_tbta_nodes.py
    python extract_tbta_nodes.py --testament NT
    python extract_tbta_nodes.py --book JHN
    python extract_tbta_nodes.py --dry-run

Algorithm:
    1. Clone/update TBTA repository from GitHub
    2. For each TBTA JSON file:
        a. Parse filename → verse reference (BOOK.chapter.verse)
        b. Load TBTA JSON and corresponding Macula YAML
        c. Flatten TBTA hierarchical tree to word-level constituents
        d. Align TBTA words to Macula words by position
        e. Extract Strong's number from Macula
        f. Extract TBTA node attributes
        g. Store: strongs_id → {node_attributes, verse_ref}
    3. Aggregate nodes per Strong's ID
    4. Deduplicate identical nodes (same attribute set)
    5. Cap verses per node (max 100, LRU)
    6. Generate YAML files per Strong's word

Output: .data/strongs/(G|H){number:04d}/(G|H){number:04d}-tbta.yaml

See: /plan/strongs-tbta-script.md for full requirements
See: /plan/strongs-tbta-script/extraction-algorithm.md for detailed algorithm
"""

import os
import sys
import json
import subprocess
from pathlib import Path
from datetime import datetime
from collections import OrderedDict, Counter, defaultdict
import argparse
import logging
import re

try:
    import yaml
except ImportError:
    print("ERROR: PyYAML not installed. Run: pip install pyyaml")
    sys.exit(1)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# Configuration
TBTA_REPO_URL = "https://github.com/AllTheWord/tbta_db_export"
TBTA_LOCAL_PATH = Path("/tmp/tbta_db_export")
TBTA_JSON_DIR = TBTA_LOCAL_PATH / "json"
DATA_DIR = Path(".data")
COMMENTARY_DIR = DATA_DIR / "commentary"
STRONGS_DIR = DATA_DIR / "strongs"

# Book name to USFM code mapping (from extract_feature.py)
BOOK_NAME_MAP = {
    "Genesis": "GEN", "Exodus": "EXO", "Leviticus": "LEV", "Numbers": "NUM", "Deuteronomy": "DEU",
    "Joshua": "JOS", "Judges": "JDG", "Ruth": "RUT",
    "1Samuel": "1SA", "1_Samuel": "1SA",
    "2Samuel": "2SA", "2_Samuel": "2SA",
    "1Kings": "1KI", "1_Kings": "1KI",
    "2Kings": "2KI", "2_Kings": "2KI",
    "1Chronicles": "1CH", "2Chronicles": "2CH",
    "Ezra": "EZR", "Nehemiah": "NEH", "Esther": "EST", "Job": "JOB", "Psalms": "PSA",
    "Proverbs": "PRO", "Ecclesiastes": "ECC", "SongofSongs": "SNG", "Isaiah": "ISA",
    "Jeremiah": "JER", "Lamentations": "LAM", "Ezekiel": "EZK", "Daniel": "DAN",
    "Hosea": "HOS", "Joel": "JOL", "Amos": "AMO", "Obadiah": "OBA", "Jonah": "JON",
    "Micah": "MIC", "Nahum": "NAM", "Habakkuk": "HAB", "Zephaniah": "ZEP",
    "Haggai": "HAG", "Zechariah": "ZEC", "Malachi": "MAL",
    "Matthew": "MAT", "Mark": "MRK", "Luke": "LUK", "John": "JHN", "Acts": "ACT",
    "Romans": "ROM", "1Corinthians": "1CO", "2Corinthians": "2CO", "Galatians": "GAL",
    "Ephesians": "EPH", "Philippians": "PHP", "Colossians": "COL",
    "1Thessalonians": "1TH", "1_Thessalonians": "1TH",
    "2Thessalonians": "2TH", "2_Thessalonians": "2TH",
    "1Timothy": "1TI", "2Timothy": "2TI", "Titus": "TIT", "Philemon": "PHM", "Hebrews": "HEB",
    "James": "JAS", "1Peter": "1PE", "2Peter": "2PE",
    "1John": "1JN", "2John": "2JN", "2_John": "2JN",
    "3John": "3JN", "Jude": "JUD",
    "Revelation": "REV", "Revelations": "REV"
}

# Fields to extract from TBTA nodes (linguistic features)
TBTA_NODE_FIELDS = [
    "Constituent", "LexicalSense", "Part", "SemanticComplexityLevel",
    "Adjective Degree", "Aspect", "Mood", "Polarity", "Time",
    "Number", "Person", "Voice", "Tense", "Case", "Gender",
    "Participant Status", "Participant Tracking", "Semantic Role"
]


def clone_tbta_repo():
    """Clone or update TBTA repository."""
    if TBTA_LOCAL_PATH.exists():
        logger.info(f"TBTA repo already exists at {TBTA_LOCAL_PATH}")

        if (TBTA_LOCAL_PATH / ".git").exists():
            logger.info("Updating TBTA repo with git pull...")
            try:
                subprocess.run(
                    ["git", "pull"],
                    cwd=TBTA_LOCAL_PATH,
                    check=True,
                    capture_output=True
                )
                logger.info("✓ TBTA repo updated")
            except subprocess.CalledProcessError as e:
                logger.warning(f"Git pull failed: {e}. Continuing with existing data...")
        else:
            logger.warning("Directory exists but is not a git repo. Using existing data...")
    else:
        logger.info(f"Cloning TBTA repo from {TBTA_REPO_URL}...")
        logger.info("This may take a few minutes (shallow clone)...")

        try:
            subprocess.run(
                ["git", "clone", "--depth=1", TBTA_REPO_URL, str(TBTA_LOCAL_PATH)],
                check=True,
                capture_output=True
            )
            logger.info("✓ TBTA repo cloned successfully")
        except subprocess.CalledProcessError as e:
            logger.error(f"Failed to clone TBTA repo: {e}")
            logger.error("Please clone manually: git clone --depth=1 " + TBTA_REPO_URL)
            sys.exit(1)

    if not TBTA_JSON_DIR.exists():
        logger.error(f"JSON directory not found: {TBTA_JSON_DIR}")
        sys.exit(1)

    logger.info(f"Using TBTA data from: {TBTA_JSON_DIR}")


def get_git_commit_hash():
    """Get current git commit hash from TBTA repo."""
    try:
        result = subprocess.run(
            ["git", "rev-parse", "HEAD"],
            cwd=TBTA_LOCAL_PATH,
            capture_output=True,
            text=True,
            check=True
        )
        return result.stdout.strip()[:7]
    except:
        return "unknown"


def parse_tbta_filename(filename: str) -> Tuple[Optional[str], Optional[int], Optional[int]]:
    """
    Parse TBTA filename like '00_001_001_Genesis.json'
    Returns (book_name, chapter, verse) tuple
    """
    import re
    match = re.match(r'(\d+)_(\d+)_(\d+)_([^.]+)\.json', filename)
    if match:
        chapter = int(match.group(2))
        verse = int(match.group(3))
        book_name = match.group(4)
        return book_name, chapter, verse
    return None, None, None


def load_macula_data(book_code: str, chapter: int, verse: int) -> Optional[Dict]:
    """
    Load Macula YAML data for a verse.
    Returns dict with word data including Strong's numbers and positions.
    """
    # Standard path: .data/commentary/JHN/001/001/JHN-001-001-macula.yaml
    macula_file = MACULA_DIR / book_code / f"{chapter:03d}" / f"{verse:03d}" / f"{book_code}-{chapter:03d}-{verse:03d}-macula.yaml"

    if not macula_file.exists():
        # Try alternate format without verse subdirectory
        macula_file = MACULA_DIR / book_code / f"{chapter:03d}" / f"{book_code}-{chapter:03d}-{verse:03d}-macula.yaml"

    if not macula_file.exists():
        return None

    try:
        with open(macula_file, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except Exception as e:
        logger.debug(f"Failed to load Macula data for {book_code} {chapter}:{verse}: {e}")
        return None


def extract_tbta_nodes_from_clause(clause_data: Dict, position_counter: List[int]) -> List[Dict]:
    """
    Recursively extract TBTA nodes from clause tree.
    Tracks word position to enable matching with Macula data.

    Args:
        clause_data: TBTA clause dictionary
        position_counter: Mutable list with single element tracking current word position

    Returns:
        List of (position, node_features) tuples
    """
    nodes = []

    if not isinstance(clause_data, dict):
        return nodes

    # Check if this is a word-level node (has "Constituent" field)
    if "Constituent" in clause_data:
        position_counter[0] += 1

        # Extract relevant linguistic features
        node_features = {}
        for field in TBTA_NODE_FIELDS:
            if field in clause_data:
                value = clause_data[field]
                # Filter out null/empty/unspecified values
                if value and value not in ["Not Applicable", "Unspecified", ".", ""]:
                    node_features[field] = str(value)

        # Only include if we have meaningful features
        if node_features:
            nodes.append({
                "position": position_counter[0],
                "features": node_features
            })

    # Recurse into children
    if "Children" in clause_data and isinstance(clause_data["Children"], list):
        for child in clause_data["Children"]:
            nodes.extend(extract_tbta_nodes_from_clause(child, position_counter))

    return nodes


def process_verse(tbta_json_file: Path) -> List[Tuple[str, str, Dict, str]]:
    """
    Process a single TBTA JSON file, linking nodes to Strong's numbers.

    Returns:
        List of (strong_prefix, strong_number, node_features, verse_ref) tuples
    """
    # Parse filename to get book/chapter/verse
    book_name, chapter, verse = parse_tbta_filename(tbta_json_file.name)
    if not book_name:
        return []

    book_code = BOOK_NAME_MAP.get(book_name)
    if not book_code:
        logger.debug(f"Unknown book name: {book_name}")
        return []

    verse_ref = f"{book_code}-{chapter:03d}-{verse:03d}"

    # Load Macula data (contains Strong's numbers)
    macula_data = load_macula_data(book_code, chapter, verse)
    if not macula_data or "words" not in macula_data:
        logger.debug(f"No Macula data for {verse_ref}")
        return []

    # Build position -> Strong's mapping from Macula
    position_to_strongs = {}
    for word in macula_data["words"]:
        if "position" in word and "lexical" in word and "strong" in word["lexical"]:
            pos = word["position"]
            strong_num = word["lexical"]["strong"]
            # Determine prefix (G for Greek, H for Hebrew)
            # Greek words have 'n' prefix in wordID (New Testament)
            # Hebrew words have 'o' prefix (Old Testament)
            word_id = word.get("ids", {}).get("wordID", "")
            prefix = "G" if word_id.startswith("n") else "H"
            position_to_strongs[pos] = (prefix, strong_num)

    # Load TBTA data
    try:
        with open(tbta_json_file, 'r', encoding='utf-8') as f:
            tbta_data = json.load(f)
    except Exception as e:
        logger.debug(f"Failed to parse {tbta_json_file.name}: {e}")
        return []

    # Extract TBTA nodes with positions
    position_counter = [0]  # Mutable counter for recursive tracking
    tbta_nodes = []

    if isinstance(tbta_data, list):
        for clause in tbta_data:
            tbta_nodes.extend(extract_tbta_nodes_from_clause(clause, position_counter))
    elif isinstance(tbta_data, dict):
        tbta_nodes.extend(extract_tbta_nodes_from_clause(tbta_data, position_counter))

    # Link TBTA nodes to Strong's numbers
    results = []
    for node in tbta_nodes:
        pos = node["position"]
        features = node["features"]

        if pos in position_to_strongs:
            prefix, strong_num = position_to_strongs[pos]
            results.append((prefix, strong_num, features, verse_ref))

    return results


def normalize_node_features(features: Dict) -> Tuple[str, ...]:
    """
    Normalize node features to a hashable tuple for uniqueness detection.
    Ensures consistent ordering of fields.
    """
    # Sort by field name for consistent ordering
    sorted_items = sorted(features.items())
    return tuple(sorted_items)


def extract_all_tbta_nodes(max_verses_per_node: int = 100, limit: Optional[int] = None):
    """
    Process all TBTA verses and aggregate nodes by Strong's number.

    Args:
        max_verses_per_node: Maximum example verses to store per unique node
        limit: Optional limit on number of verses to process (for testing)

    Returns:
        Dict mapping Strong's IDs to their TBTA nodes
    """
    logger.info("=" * 60)
    logger.info("TBTA NODE EXTRACTION")
    logger.info("=" * 60)

    # Data structure: strongs_id -> list of unique nodes
    # Each node has: features dict + list of example verses
    strongs_nodes = defaultdict(lambda: defaultdict(list))

    # Get all JSON files
    json_files = sorted(TBTA_JSON_DIR.glob("*.json"))
    total_files = len(json_files)

    if limit:
        json_files = json_files[:limit]
        logger.info(f"Processing first {limit} of {total_files} files (limit mode)")
    else:
        logger.info(f"Processing {total_files} TBTA verse files")

    # Process each verse
    processed = 0
    nodes_found = 0

    for json_file in json_files:
        results = process_verse(json_file)

        for prefix, strong_num, features, verse_ref in results:
            strongs_id = f"{prefix}{int(strong_num):04d}"

            # Normalize features for uniqueness detection
            feature_key = normalize_node_features(features)

            # Add verse to this node configuration (capped)
            if len(strongs_nodes[strongs_id][feature_key]) < max_verses_per_node:
                strongs_nodes[strongs_id][feature_key].append(verse_ref)

            nodes_found += 1

        processed += 1
        if processed % 1000 == 0:
            logger.info(f"  Processed {processed}/{len(json_files)} files... ({nodes_found} node instances)")

    logger.info(f"✓ Processed {processed} files")
    logger.info(f"✓ Found {nodes_found} total node instances")
    logger.info(f"✓ Mapped to {len(strongs_nodes)} unique Strong's words")

    return strongs_nodes


def write_strongs_yaml_files(strongs_nodes: Dict, max_verses_per_node: int, dry_run: bool = False):
    """
    Write YAML files for each Strong's word containing its TBTA nodes.

    Format: .data/strongs/(G|H){number:04d}/{number}-tbta.yaml
    """
    logger.info("=" * 60)
    logger.info("WRITING YAML FILES")
    logger.info("=" * 60)

    if dry_run:
        logger.info("DRY RUN MODE - No files will be written")

    files_written = 0
    total_nodes = 0

    for strongs_id, nodes_dict in sorted(strongs_nodes.items()):
        # Parse Strong's ID (e.g., "G1510" or "H0430")
        prefix = strongs_id[0]  # G or H
        number = strongs_id[1:]  # e.g., "1510"

        # Build output path
        output_dir = STRONGS_OUTPUT_DIR / strongs_id
        output_file = output_dir / f"{strongs_id}-tbta.yaml"

        # Build node list
        nodes_list = []
        for feature_tuple, verses in nodes_dict.items():
            # Convert normalized tuple back to dict
            features = dict(feature_tuple)

            node_data = {**features}  # Copy all features
            node_data["Verses"] = verses
            nodes_list.append(node_data)
            total_nodes += 1

        # Build YAML structure
        yaml_data = {
            "source": "tbta",
            "tbta_commit": get_git_commit_hash(),
            "extracted": datetime.utcnow().isoformat() + "Z",
            "strongs": strongs_id,
            "max_verses_per_node": max_verses_per_node,
            "total_unique_nodes": len(nodes_list),
            "nodes": nodes_list
        }

        if not dry_run:
            # Create directory
            output_dir.mkdir(parents=True, exist_ok=True)

            # Write YAML
            with open(output_file, 'w', encoding='utf-8') as f:
                yaml.dump(
                    yaml_data,
                    f,
                    default_flow_style=False,
                    allow_unicode=True,
                    sort_keys=False
                )

            files_written += 1
            if files_written % 100 == 0:
                logger.info(f"  Written {files_written} files...")
        else:
            logger.info(f"  Would write: {output_file} ({len(nodes_list)} nodes)")

    logger.info("=" * 60)
    logger.info(f"✓ {'Would write' if dry_run else 'Written'} {files_written} YAML files")
    logger.info(f"✓ Total unique TBTA nodes: {total_nodes}")
    logger.info("=" * 60)


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Extract TBTA nodes and link to Strong's numbers",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python extract_tbta_nodes.py
  python extract_tbta_nodes.py --max-verses-per-node 50
  python extract_tbta_nodes.py --dry-run
  python extract_tbta_nodes.py --limit 100  # Test with first 100 verses
        """
    )

    parser.add_argument(
        "--max-verses-per-node",
        type=int,
        default=100,
        help="Maximum example verses per unique node configuration (default: 100)"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Show statistics without writing files"
    )
    parser.add_argument(
        "--limit",
        type=int,
        help="Limit processing to first N verses (for testing)"
    )

    args = parser.parse_args()

    # Verify data directory exists
    if not DATA_DIR.exists():
        logger.error(f"Data directory not found: {DATA_DIR}")
        logger.error("Please run: ./setup-minimal-data.sh")
        sys.exit(1)

    # Clone/update TBTA repo
    clone_tbta_repo()

    # Extract nodes
    strongs_nodes = extract_all_tbta_nodes(
        max_verses_per_node=args.max_verses_per_node,
        limit=args.limit
    )

    # Write output files
    write_strongs_yaml_files(
        strongs_nodes,
        max_verses_per_node=args.max_verses_per_node,
        dry_run=args.dry_run
    )

    logger.info("✓ TBTA extraction complete!")


if __name__ == "__main__":
    main()
